{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "import csv\n",
    "import SimpleITK as sitk\n",
    "#from lungtumormask import mask as tumormask\n",
    "#from lungmask import mask as lungmask_fun\n",
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    RandFlipd,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    FillHoles,\n",
    "    RemoveSmallObjects,\n",
    "    KeepLargestConnectedComponent,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    CenterSpatialCropd,\n",
    "    SpatialCropd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    AsDiscrete,\n",
    "    SpatialCrop,\n",
    "    RandSpatialCropd,\n",
    "    SpatialPadd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd,\n",
    "    DivisiblePadd,\n",
    "    MapTransform,\n",
    "    RandWeightedCropd,\n",
    "    ToTensord,\n",
    "    Transpose,\n",
    "    ToTensor,\n",
    ")\n",
    "from monai.optimizers import LearningRateFinder\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet,VNet,SwinUNETR,UNETR,DynUNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric,SurfaceDiceMetric,SurfaceDistanceMetric,HausdorffDistanceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch,pad_list_data_collate\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "\n",
    "\n",
    "if False:\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#peregrine\n",
    "#root_path = '/data/p308104/NBIA_Data/NIFTI_NBIA/imagesTr/'\n",
    "#root_path = '/data/p308104/Nifti_Imgs_V0/' #UMCG data on peregrine\n",
    "#local\n",
    "#root_path = '/home/umcg/Desktop/NBIA/NBIA_Nifti_v0/imagesTr/'\n",
    "#root_path = '/home/umcg/Desktop/Dicom_UMCG/MultipleBP/' \n",
    "#root_path = '/home/umcg/OneDrive/MultipleBreathingP/'\n",
    "root_path = '/home/umcg/Desktop/AutomaticITV_code/MultipleBreathingP-OneDriveCopy/MultipleBreathingP/'\n",
    "\n",
    "\n",
    "all_patientdir = []\n",
    "all_patientdir = os.listdir(root_path)\n",
    "all_patientdir.sort()\n",
    "print(len(all_patientdir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_PxOk = []\n",
    "list_PxNOTOk = []\n",
    "CT_fpaths = []\n",
    "lbl_fpaths= []\n",
    "lung_fpaths = []\n",
    "list_ct_names = [\"50%_ct.nii.gz\",\"ex_ct.nii.gz\",\"in_ct.nii.gz\",\"mar_ct.nii.gz\"]\n",
    "list_lung_names = [\"50%_lung_mask.nii.gz\",\"ex_lung_mask.nii.gz\",\"in_lung_mask.nii.gz\",\"mar_lung_mask.nii.gz\"]\n",
    "for patient_path in all_patientdir:\n",
    "    flag_PxOk=0\n",
    "    ct_miss = True\n",
    "    lung_miss = True\n",
    "    gtv_miss = True\n",
    "    for root, dirs, files in os.walk(root_path+patient_path, topdown=False):   \n",
    "        for f in files:\n",
    "            if \"50%_ct.nii.gz\" in f.lower() and ct_miss:\n",
    "                CT_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                flag_PxOk+=1\n",
    "                ct_miss = False\n",
    "            if \"ex_ct.nii.gz\" in f.lower() and ct_miss:\n",
    "                CT_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                flag_PxOk+=1\n",
    "                ct_miss = False\n",
    "            if \"in_ct.nii.gz\" in f.lower() and ct_miss:\n",
    "                CT_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                flag_PxOk+=1\n",
    "                ct_miss = False\n",
    "            if \"mar_ct.nii.gz\" in f.lower() and ct_miss:\n",
    "                CT_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                flag_PxOk+=1\n",
    "                ct_miss = False\n",
    "            if \"gtv.nii.gz\" in f.lower():\n",
    "                lbl_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                flag_PxOk+=1\n",
    "                gtv_miss =False\n",
    "            if (\"50%_lungmask.nii.gz\" in f.lower()) and lung_miss:\n",
    "                lung_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                lung_miss = False\n",
    "                flag_PxOk+=1\n",
    "            if (\"ex_lungmask.nii.gz\" in f.lower()) and lung_miss:\n",
    "                lung_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                flag_PxOk+=1\n",
    "                lung_miss = False\n",
    "            if (\"in_lungmask.nii.gz\" in f.lower()) and lung_miss:\n",
    "                lung_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                flag_PxOk+=1\n",
    "                lung_miss = False\n",
    "            if (\"mar_lungmask.nii.gz\" in f.lower()) and lung_miss:\n",
    "                lung_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                flag_PxOk+=1\n",
    "                lung_miss = False\n",
    "\n",
    "        if flag_PxOk!=3:\n",
    "            list_PxNOTOk.append(patient_path)\n",
    "            if ct_miss==False:\n",
    "                print(patient_path,flag_PxOk,'GTV Miss: ',gtv_miss,'CT miss: ',ct_miss)\n",
    "                CT_fpaths = CT_fpaths[:-1]\n",
    "                lung_fpaths =lung_fpaths[:-1] \n",
    "        else:\n",
    "            list_PxOk.append(patient_path)\n",
    "                \n",
    "    \n",
    "\n",
    "print(len(CT_fpaths),len(lbl_fpaths),len(lung_fpaths))\n",
    "CT_fpaths.sort()\n",
    "lbl_fpaths.sort()\n",
    "lung_fpaths.sort()\n",
    "\n",
    "print(CT_fpaths[44])\n",
    "print(lbl_fpaths[44])\n",
    "print(lung_fpaths[44])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save csv file of patients ok and px not ok \n",
    "if False:\n",
    "    print('List Ok: ',len(list_PxOk),'List NOT Ok',len(list_PxNOTOk))\n",
    "    file = open('ListPatientsStatus.csv', 'w')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Patients Ok'])\n",
    "    writer.writerow(list_PxOk)\n",
    "    writer.writerow(['Patients NOT Ok'])\n",
    "    writer.writerow(list_PxNOTOk)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Predicts both tumor and lung mask\n",
    "Predict = False\n",
    "w = '50%'\n",
    "if Predict:\n",
    "    gtv = lbl_fpaths[0]\n",
    "    gtv = nib.load(gtv)\n",
    "\n",
    "    for ct in CT_fpaths:\n",
    "        empty_header = nib.Nifti1Header()\n",
    "        predicted_path = root_path+ct+'_'+w+'_predictedTS.nii.gz'\n",
    "        lung_path = ct[:-10]+'_LungMask.nii.gz'\n",
    "        #Get Tumor mask\n",
    "        #tumormask.mask(ct, predicted_path)\n",
    "        #Get Lung mask and save it\n",
    "        input_image = sitk.ReadImage(ct, imageIO='NiftiImageIO')\n",
    "        lungmask = lungmask_fun.apply(input_image)  # default model is U-net(R231)\n",
    "        lungmask_ni = nib.Nifti1Image(lungmask, gtv.affine,empty_header)\n",
    "        nib.save(lungmask_ni,lung_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check max min tumor values\n",
    "if False :\n",
    "    minmin = 10\n",
    "    maxmax = 0\n",
    "    for i in range(len(data_dicts)):\n",
    "        img = nib.load(data_dicts[i][\"lung\"])\n",
    "        data_img = img.get_fdata()\n",
    "        data_img = np.transpose(data_img, (1,2,0))# fix misshape \n",
    "        for j in range(data_img.shape[2]):\n",
    "            if np.sum(data_img[:,:,j])>0:\n",
    "                plt.imshow(data_img[:,:,j])\n",
    "                plt.show()\n",
    "        break\n",
    "        if np.min(data_img) < minmin:\n",
    "          minmin = np.min(data_img)\n",
    "        if np.max(data_img) > maxmax:\n",
    "          maxmax = np.max(data_img)\n",
    "\n",
    "    print(\"minmin: \",minmin,\"maxmax: \", maxmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data dictionat\n",
    "data_dicts = [\n",
    "    {\"image\": image_name,\"lung\":lung_name,\"label\": label_name}\n",
    "    for image_name,lung_name,label_name in zip(CT_fpaths,lung_fpaths,lbl_fpaths)\n",
    "]\n",
    "train_files, val_files = data_dicts[:-100], data_dicts[-100:]\n",
    "print('train val len:',len(train_files),'-',len(val_files))\n",
    "\n",
    "minmin_CT = -1024 #NBIA\n",
    "maxmax_CT = 3071 #NBIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to transpose lung mask\n",
    "class Create_sequences(MapTransform):\n",
    "    def __init__(self, keys):\n",
    "        super().__init__(keys)\n",
    "        \n",
    "        print(f\"keys to transpose: {self.keys}\")\n",
    "\n",
    "         \n",
    "    def __call__(self, dictionary):\n",
    "        dictionary = dict(dictionary)\n",
    "        for key in self.keys:\n",
    "            data = dictionary[key]\n",
    "            if key == 'lung':\n",
    "                data = np.transpose(data, (0,2,3,1))\n",
    "                data = rotate(data,270,axes=(1,2),reshape=False)\n",
    "                data = np.flip(data,1)\n",
    "                data[data==2] = int(1)\n",
    "                data[data!=1] = int(0)\n",
    "            dictionary[key] = data\n",
    "            \n",
    "        return dictionary        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Compose functions for preprocessing of train and validation\n",
    "set_determinism(seed=0)\n",
    "image_keys = [\"image\",\"lung\",\"label\"]\n",
    "p = .5 #Data aug transform probability\n",
    "size = 96\n",
    "image_size = (size,size,size)\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=image_keys),\n",
    "        EnsureChannelFirstd(keys=image_keys),\n",
    "        Orientationd(keys=[\"image\",\"label\"], axcodes=\"RAS\"),\n",
    "        #Spacingd(keys=[\"image\",\"label\"], pixdim=(1,1,1),mode=(\"bilinear\",\"nearest\")),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=minmin_CT, a_max=maxmax_CT,b_min=0.0, b_max=1.0, clip=True,),\n",
    "        Create_sequences(keys=image_keys),\n",
    "        CropForegroundd(keys=image_keys, source_key=\"lung\",k_divisible = size),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=image_keys,label_key='label',spatial_size=image_size,pos=1,neg=1,num_samples=2,\n",
    "            image_key='image',image_threshold=0,),\n",
    "        #RandFlipd(keys=image_keys,spatial_axis=[0],prob=p / 3,),\n",
    "        #RandFlipd(keys=image_keys,spatial_axis=[1],prob=p / 3,),\n",
    "        #RandFlipd(keys=image_keys,spatial_axis=[2],prob=p / 3,),\n",
    "        ToTensord(keys=image_keys),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=image_keys),\n",
    "        EnsureChannelFirstd(keys=image_keys),\n",
    "        Orientationd(keys=[\"image\",\"label\"], axcodes=\"RAS\"),\n",
    "        #Spacingd(keys=[\"image\",\"label\"], pixdim=(1,1,1),mode=(\"bilinear\",\"nearest\")),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=minmin_CT, a_max=maxmax_CT,b_min=0.0, b_max=1.0, clip=True,),\n",
    "        Create_sequences(keys=image_keys),\n",
    "        CropForegroundd(keys=image_keys, source_key=\"lung\",k_divisible = size),\n",
    "        ToTensord(keys=image_keys),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Check the images after the preprocessing\n",
    "if True:\n",
    "    check_ds =Dataset(data=val_files, transform=val_transforms)\n",
    "    check_loader = DataLoader(check_ds, batch_size=1,num_workers=0)\n",
    "    if False:\n",
    "        count = 1\n",
    "        for batch_data in check_loader:\n",
    "            #batch_data = first(check_loader)\n",
    "            image,lung, label = (batch_data[\"image\"][0][0],batch_data[\"lung\"][0][0],batch_data[\"label\"][0][0])\n",
    "            print(f\"px info:{count },image shape: {image.shape},lung shape: {lung.shape}, label shape: {label.shape}\")\n",
    "            count+=1\n",
    "            for i in range(label.shape[2]):\n",
    "                if torch.sum(label[:,:,i])>0:\n",
    "                    plt.subplot(1,3,1),plt.imshow(image[:,:,i]),plt.axis('off')\n",
    "                    plt.subplot(1,3,2),plt.imshow(label[:,:,i]+lung[:,:,i]),plt.axis('off')\n",
    "                    plt.subplot(1,3,3),plt.imshow(label[:,:,i]+image[:,:,i]),plt.axis('off')\n",
    "                    count+=1\n",
    "                    plt.tight_layout(),plt.show()\n",
    "                    break\n",
    "            if count>10:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the dataset ready for the model\n",
    "if False:\n",
    "    train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "    val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, num_workers=0)#,collate_fn=pad_list_data_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create the model\n",
    "spatial_dims = 3\n",
    "max_epochs = 100\n",
    "in_channels = 1\n",
    "out_channels=2 #including background\n",
    "if True:\n",
    "    model = SwinUNETR(\n",
    "        image_size, \n",
    "        in_channels, out_channels, \n",
    "        use_checkpoint=True, \n",
    "        feature_size=24,\n",
    "        #spatial_dims=spatial_dims\n",
    "    ).to(device)\n",
    "else:\n",
    "    model = UNet(\n",
    "        spatial_dims=spatial_dims,\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        strides=(2, 2, 2, 2),\n",
    "        num_res_units=4,\n",
    "        norm=Norm.BATCH,\n",
    "    ).to(device)\n",
    "\n",
    "#metrics\n",
    "loss_function = DiceLoss(to_onehot_y=True, sigmoid=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "surfDice_metric = SurfaceDiceMetric([0.01,0.01],include_background=True)\n",
    "surfDice_metric_1Class = SurfaceDiceMetric([10],include_background=False)\n",
    "hausdorff_metric = HausdorffDistanceMetric(include_background=True, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "pretrained_path = '/home/umcg/Desktop/AutomaticITV_code/weights/best_m_MONAI_V3_NBIAWeightsretrainedWithUMCGdata.pth'\n",
    "\n",
    "if pretrained_path is not(None):\n",
    "    model.load_state_dict(torch.load(pretrained_path, map_location=torch.device(device)))\n",
    "\n",
    "    #weight = torch.load(pretrained_path, map_location=torch.device(device))\n",
    "    #model.load_from(weights=weight)\n",
    "    print('Using pretrained weights!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "post_transforms = Compose(\n",
    "    [\n",
    "        EnsureType(),\n",
    "        AsDiscrete(argmax=True,to_onehot=out_channels, threshold=.5),\n",
    "        FillHoles(applied_labels=1, connectivity=0),\n",
    "        RemoveSmallObjects(min_size=5, connectivity=3, independent_channels=True),\n",
    "        #KeepLargestConnectedComponent(applied_labels=None,is_onehot=True,connectivity=3, num_components=3),\n",
    "   ]\n",
    ")\n",
    "\n",
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True,threshold=0.5)])\n",
    "post_label = Compose([EnsureType(), AsDiscrete(threshold=0.5)],)\n",
    "\n",
    "#print(postImg_monai[0].shape)\n",
    "\n",
    "#plt.subplot(1,2,1),plt.imshow(val_outputs[0,1,:,:,104].cpu())\n",
    "#plt.subplot(1,2,2),plt.imshow(postImg_monai[0][1,:,:,104].cpu())\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#compute metric for current iteration\n",
    "#Batch x Channel x Height x Width - [B,C,H,W] - 1, 384, 288, 192\n",
    "#Channel is number of classes\n",
    "def surfDiceFun(val_labels,postImg_monai):\n",
    "    ylabe_BCHW_neg = val_labels[0].cpu().numpy()\n",
    "    ylabe_BCHW_neg = ylabe_BCHW_neg*-1+1\n",
    "    ylabe_BCHW_pos = val_labels[0].cpu().numpy()\n",
    "    ylabe_BCHW = np.concatenate((ylabe_BCHW_neg, ylabe_BCHW_pos),0)\n",
    "    transpose_monai = Compose([Transpose([3, 0, 1, 2])])\n",
    "    ylabe_BCHW = transpose_monai(ylabe_BCHW)\n",
    "    ypred_BCHW = postImg_monai[0].permute(3, 0, 1, 2)\n",
    "\n",
    "    list_surf=[]\n",
    "    for i in range(ypred_BCHW.shape[0]-4):\n",
    "        surfDice_metric(y_pred=ypred_BCHW[i:i+2,:,:,:], y=ylabe_BCHW[i:i+2,:,:,:])\n",
    "        #print('SurfFice monai    :',surfDice_metric.aggregate().item())\n",
    "        list_surf.append(surfDice_metric.aggregate().item())\n",
    "        surfDice_metric.reset()\n",
    "    return np.mean(list_surf)\n",
    "\n",
    "def surfDiceFun_1Class(val_labels,postImg_monai):\n",
    "    ylabe_BCHW_neg = val_labels[0].cpu().numpy()\n",
    "    ylabe_BCHW_neg = ylabe_BCHW_neg*-1+1\n",
    "    ylabe_BCHW_pos = val_labels[0].cpu().numpy()\n",
    "    ylabe_BCHW = np.concatenate((ylabe_BCHW_neg, ylabe_BCHW_pos),0)\n",
    "    transpose_monai = Compose([Transpose([3, 0, 1, 2])])\n",
    "    ylabe_BCHW = transpose_monai(ylabe_BCHW)\n",
    "    ypred_BCHW = postImg_monai[0].permute(3, 0, 1, 2)\n",
    "    \n",
    "    #print(ypred_BCHW.shape,ylabe_BCHW.shape)\n",
    "    list_surf=[]\n",
    "    for i in range(ypred_BCHW.shape[0]-4):\n",
    "        if  np.sum(ylabe_BCHW[i:i+2,1,:,:])>0:# or np.sum(ypred_BCHW[i:i+2,0,:,:])>0:\n",
    "            surfDice_metric_1Class(y_pred=ypred_BCHW[i:i+2,:,:,:], y=ylabe_BCHW[i:i+2,0:,:,:])\n",
    "            list_surf.append(surfDice_metric_1Class.aggregate().item())\n",
    "            #print(list_surf[-1])\n",
    "            surfDice_metric_1Class.reset()\n",
    "    return np.mean(list_surf)\n",
    "#hausdorff_metric(y_pred=postImg_monai, y=val_labels)\n",
    "#print('hausdorff monai    :',hausdorff_metric.aggregate().item())\n",
    "#hausdorff_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Testing the model\n",
    "count=0\n",
    "nr_images=8\n",
    "figsize = (18, 12)\n",
    "#figures_folder_i = '/data/p308104/NBIA_Data/NIFTI_NBIA/Res-16092022/' #Peregrine\n",
    "figures_folder_i ='/home/umcg/Desktop/AutomaticITV_code/figures_folder_UMCG/' #Local\n",
    "all_metrics = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for val_data in check_loader:\n",
    "        val_inputs, val_labels = (\n",
    "            val_data[\"image\"].to(device),\n",
    "            val_data[\"label\"].to(device),)\n",
    "        roi_size = image_size\n",
    "        sw_batch_size = 1\n",
    "        val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "        \n",
    "        #postImg_manual  = PostProcessing(val_outputs.cpu())\n",
    "        #postImg_manual  = [post_transforms(i) for i in decollate_batch(postImg_manual)]\n",
    "        \n",
    "        postImg_monai = [post_transforms(i) for i in decollate_batch(val_outputs)]\n",
    "        print('read')\n",
    "        if True: #True if plot\n",
    "            # Determine slices to be plotted\n",
    "            # Make sure that nr_images that we want to plot is greater than or equal to the number of slices available\n",
    "            slice_indices = []\n",
    "            for i in range(val_inputs.shape[4]):\n",
    "                if np.sum(val_labels.cpu().numpy()[0, 0, :, :, i])>0:\n",
    "                    slice_indices.append(i)\n",
    "            if len(slice_indices) <nr_images:\n",
    "                slice_indices.append(random.sample(range(1, 95),nr_images-len(slice_indices))[0])\n",
    "            else:\n",
    "                slice_indices = random.sample(slice_indices, k=nr_images)\n",
    "            instance = random.randint(0, val_inputs.shape[0] - 1)\n",
    "            j = 1\n",
    "            px = val_files[count]['label'].split('/')[-2]\n",
    "            for i, idx in enumerate(slice_indices):\n",
    "                j=1+i\n",
    "                fig = plt.figure('Instance = {}'.format(instance), figsize=figsize)\n",
    "                plt.subplot(4, nr_images, j),plt.title('CT ({})'.format(idx)),plt.imshow(val_inputs.cpu().numpy()[instance, 0, :, :, idx], cmap='gray', vmin=0, vmax=1),plt.axis('off')\n",
    "                plt.subplot(4, nr_images, nr_images+j),plt.title('GTV ({})'.format(idx)),plt.imshow(val_labels.cpu().numpy()[instance, 0, :, :, idx]),plt.axis('off')\n",
    "                plt.subplot(4, nr_images, 2*nr_images+j),plt.title('Prediction ({})'.format(idx)),plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[instance, :, :, idx]),plt.axis('off')\n",
    "                #plt.subplot(5, nr_images, j),plt.title('postImg Manual ({})'.format(idx)),plt.imshow(postImg_manual[0].detach().cpu()[instance, :, :, idx]),plt.axis('off')\n",
    "                plt.subplot(4, nr_images, 3*nr_images+j),plt.title('postImg Monai ({})'.format(idx)),plt.imshow(postImg_monai[0].detach().cpu()[1, :, :, idx]),plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(figures_folder_i,'final_{}.png'.format(px)))\n",
    "        val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "        val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "\n",
    "        #compute metric for current iteration\n",
    "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "        dice0 = dice_metric.aggregate().item()\n",
    "        print('Scores vanilla:',dice0)\n",
    "        dice_metric.reset()\n",
    "\n",
    "        dice_metric(y_pred=postImg_monai[0][1:2,:,:,:], y=val_labels[0])\n",
    "        dice1 = dice_metric.aggregate().item()\n",
    "        print('Scores monai    :',dice1)\n",
    "        dice_metric.reset()\n",
    "        sdice0 = surfDiceFun_1Class(val_labels,val_outputs)\n",
    "        sdice1 = surfDiceFun_1Class(val_labels,postImg_monai)\n",
    "        print('SurfFice vanilla 1 class:',sdice0)\n",
    "        print('SurfFice monai 1 class:',sdice1)\n",
    "        all_metrics.append([val_files[count]['label'].split('/')[-2],dice0,dice1,sdice0,sdice1])\n",
    "        count+=1\n",
    "        \n",
    "#Save csv of metrics\n",
    "if True: \n",
    "    f= open(figures_folder_i+'res_UMCG.csv','w', encoding='UTF8')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Patient','Dice V','Dice P','Sdice V','SDice P'])\n",
    "    writer.writerows(all_metrics)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Save csv of metrics\n",
    "f= open(figures_folder_i+'res','w', encoding='UTF8')\n",
    "writer = csv.writer(f)\n",
    "writer.writerow(['Patient','Dice V','Dice P','Sdice V','SDice P'])\n",
    "writer.writerows(all_metrics)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load csv of metrics\n",
    "\n",
    "if False:\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(r'/home/umcg/Desktop/AutomaticITV_code/figures_folder_i/res.csv')\n",
    "    print(df.mean())\n",
    "    print(df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "      hausdorff_metric(y_pred=postImg_monai, y=val_labels)\n",
    "        print('hausdorff monai    :',hausdorff_metric.aggregate().item())\n",
    "        hausdorff_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import dilation,disk,erosion\n",
    "def PostProcessing(predicted):\n",
    "    img = predicted\n",
    "    for i in range(predicted.shape[-1]-10):\n",
    "        j=i+5\n",
    "        #Get imgs\n",
    "        Before = predicted[0,1,:,:,j-1]\n",
    "        Current = predicted[0,1,:,:,j]\n",
    "        After = predicted[0,1,:,:,j+1]\n",
    "        AAfter = predicted[0,1,:,:,j+2]+After\n",
    "        AAfter = predicted[0,1,:,:,j+3]+AAfter\n",
    "        AAfter = predicted[0,1,:,:,j+4]+AAfter\n",
    "        AAfter = predicted[0,1,:,:,j+5]+AAfter\n",
    "        BBefore = predicted[0,1,:,:,j-2]+Before\n",
    "        BBefore = predicted[0,1,:,:,j-3]+BBefore\n",
    "        BBefore = predicted[0,1,:,:,j-4]+BBefore\n",
    "        BBefore = predicted[0,1,:,:,j-5]+BBefore\n",
    "        \n",
    "        #Binarize\n",
    "        t = .5\n",
    "        BBefore[BBefore>=t] =int(1)\n",
    "        BBefore[BBefore<t] =int(0)\n",
    "        Current[Current>=t] =int(1)\n",
    "        Current[Current<t] =int(0)\n",
    "        AAfter[AAfter>=t] =int(1)\n",
    "        AAfter[AAfter<t] =int(0)\n",
    "        \n",
    "        #Morphology\n",
    "        m = 15\n",
    "        BBefore = dilation(BBefore, disk(m))\n",
    "        Current = dilation(Current, disk(m))\n",
    "        AAfter = dilation(AAfter, disk(m))\n",
    "        m = 5\n",
    "        BBefore = erosion(BBefore, disk(m))\n",
    "        Current = erosion(Current, disk(m))\n",
    "        AAfter = erosion(AAfter, disk(m))\n",
    "        \n",
    "        #And Between slices -> <- ... for missing \n",
    "        ABS = np.logical_and(BBefore,AAfter)\n",
    "        Current = np.logical_or(ABS,Current)\n",
    "        \n",
    "        #And outside slices <-->  ... for false positives with RegionProps\n",
    "        BBefore_label = label(BBefore)\n",
    "        Current_label = label(Current)\n",
    "        AAfter_label = label(AAfter)\n",
    "        BBefore_regions = regionprops(BBefore_label)\n",
    "        Current_regions = regionprops(Current_label)\n",
    "        AAfter_regions = regionprops(AAfter_label)\n",
    "        #Look for centroids distance\n",
    "        limit =  20\n",
    "        if False:\n",
    "            for c_centroid in Current_regions:\n",
    "                for b_centroid in BBefore_regions:\n",
    "                    dif = np.abs(np.asarray(c_centroid['centroid'])-np.asarray(b_centroid['centroid']))\n",
    "                    if dif[0] > limit or dif[1] > limit: \n",
    "                        Current[Current==c_centroid['label']] = int(0)\n",
    "                        print('delete blob')\n",
    "                for a_centroid in AAfter_regions:\n",
    "                    dif = np.abs(np.asarray(c_centroid['centroid'])-np.asarray(a_centroid['centroid']))\n",
    "                    if dif[0] > limit or dif[1] > limit: \n",
    "                        Current[Current==c_centroid['label']] = int(0)\n",
    "                        print('delete blob')\n",
    "        if False:\n",
    "            plt.subplot(1,4,1),plt.imshow(BBefore,cmap='gray', vmin=0, vmax=1)\n",
    "            plt.subplot(1,4,2),plt.imshow(Current,cmap='gray', vmin=0, vmax=1)\n",
    "            plt.subplot(1,4,3),plt.imshow(predicted[0,1,:,:,j],cmap='gray', vmin=0, vmax=1)\n",
    "            plt.subplot(1,4,4),plt.imshow(AAfter,cmap='gray', vmin=0, vmax=1)\n",
    "            plt.show()\n",
    "    \n",
    "        img[0,0,:,:,j] = torch.from_numpy(Current)\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
