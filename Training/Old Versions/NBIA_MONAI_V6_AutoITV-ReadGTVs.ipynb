{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "import csv\n",
    "import SimpleITK as sitk\n",
    "#from lungtumormask import mask as tumormask\n",
    "#from lungmask import mask as lungmask_fun\n",
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    RandFlipd,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    SqueezeDimd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    FillHoles,\n",
    "    RemoveSmallObjects,\n",
    "    KeepLargestConnectedComponent,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    CenterSpatialCropd,\n",
    "    SpatialCropd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    AsDiscrete,\n",
    "    SpatialCrop,\n",
    "    RandSpatialCropd,\n",
    "    SpatialPadd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd,\n",
    "    DivisiblePadd,\n",
    "    MapTransform,\n",
    "    RandWeightedCropd,\n",
    "    ToTensord,\n",
    "    Transpose,\n",
    "    ToTensor,\n",
    ")\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet,VNet,SwinUNETR,UNETR,DynUNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric,SurfaceDiceMetric,SurfaceDistanceMetric,HausdorffDistanceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch,pad_list_data_collate\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "\n",
    "if False:\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n"
     ]
    }
   ],
   "source": [
    "#peregrine\n",
    "#root_path = '/data/p308104/NBIA_Data/NIFTI_NBIA/imagesTr/'\n",
    "#root_path = '/data/p308104/Nifti_Imgs_V0/' #UMCG data on peregrine\n",
    "#local\n",
    "#root_path = '/home/umcg/Desktop/NBIA/NBIA_Nifti_v0/imagesTr/'\n",
    "#root_path = '/home/umcg/Desktop/Dicom_UMCG/MultipleBP/' \n",
    "#root_path = '/home/umcg/OneDrive/MultipleBreathingP/'\n",
    "root_path = '/home/umcg/Desktop/AutomaticITV_code/MultipleBreathingP-OneDriveCopy/MultipleBreathingP/'\n",
    "\n",
    "all_patientdir = []\n",
    "all_patientdir = os.listdir(root_path)\n",
    "all_patientdir.sort()\n",
    "print(len(all_patientdir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cts:  10 10 10\n",
      "/home/umcg/Desktop/AutomaticITV_code/MultipleBreathingP-OneDriveCopy/MultipleBreathingP/0045632/0045632_50%_LungMask.nii.gz\n",
      "/home/umcg/Desktop/AutomaticITV_code/MultipleBreathingP-OneDriveCopy/MultipleBreathingP/0045632/0045632_50%_ct.nii.gz\n",
      "/home/umcg/Desktop/AutomaticITV_code/MultipleBreathingP-OneDriveCopy/MultipleBreathingP/0045632/0045632_50%_aGTV.nii.gz\n"
     ]
    }
   ],
   "source": [
    "CT_fpaths=[]\n",
    "lbl_fpaths=[]\n",
    "lung_fpaths=[]\n",
    "patient_ok='X'\n",
    "for patient_path in all_patientdir:\n",
    "    ct_miss = True\n",
    "    lung_miss = True\n",
    "    gtv_miss = True\n",
    "    for root, dirs, files in os.walk(root_path+patient_path, topdown=True):\n",
    "        if len(files)>18:\n",
    "            for f in files:\n",
    "                if \"_agtv\" in f.lower():\n",
    "                    lbl_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    patient_ok = patient_path\n",
    "        \n",
    "    for root, dirs, files in os.walk(root_path+patient_path, topdown=True):\n",
    "        for f in files:\n",
    "            if patient_ok in f.lower():\n",
    "                if \"%_ct.nii.gz\" in f.lower():\n",
    "                    CT_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                if \"%_lungmask.nii.gz\" in f.lower():\n",
    "                    lung_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "        \n",
    "print('Cts: ',len(CT_fpaths),len(lung_fpaths),len(lbl_fpaths))\n",
    "CT_fpaths.sort()\n",
    "lung_fpaths.sort()\n",
    "lbl_fpaths.sort()\n",
    "\n",
    "num = 1000\n",
    "print(lung_fpaths[5])\n",
    "print(CT_fpaths[5])\n",
    "print(lbl_fpaths[5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 512, 512) (512, 512, 176) (1, 288, 192, 192)\n",
      "(176, 512, 512) (512, 512, 176) (1, 288, 192, 192)\n",
      "(176, 512, 512) (512, 512, 176) (1, 288, 192, 192)\n",
      "(176, 512, 512) (512, 512, 176) (1, 288, 192, 192)\n",
      "(176, 512, 512) (512, 512, 176) (1, 288, 192, 192)\n",
      "(176, 512, 512) (512, 512, 176) (1, 288, 192, 192)\n",
      "(176, 512, 512) (512, 512, 176) (1, 288, 192, 192)\n",
      "(176, 512, 512) (512, 512, 176) (1, 288, 192, 192)\n",
      "(176, 512, 512) (512, 512, 176) (1, 288, 192, 192)\n",
      "(176, 512, 512) (512, 512, 176) (1, 288, 192, 192)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lung_fpaths)):\n",
    "    lung_i = nib.load(lung_fpaths[i])\n",
    "    ct_i = nib.load(CT_fpaths[i])\n",
    "    gtv_i = nib.load(lbl_fpaths[i])\n",
    "    \n",
    "    print(lung_i.shape,ct_i.shape,gtv_i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save csv file of patients ok and px not ok \n",
    "if False:\n",
    "    print('List Ok: ',len(list_PxOk),'List NOT Ok',len(list_PxNOTOk))\n",
    "    file = open('ListPatientsStatus.csv', 'w')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Patients Ok'])\n",
    "    writer.writerow(list_PxOk)\n",
    "    writer.writerow(['Patients NOT Ok'])\n",
    "    writer.writerow(list_PxNOTOk)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Predicts both tumor and lung mask\n",
    "Predict = False\n",
    "w = '50%'\n",
    "if Predict:\n",
    "    gtv = '/home/umcg/Desktop/AutomaticITV_code/MultipleBreathingP-OneDriveCopy/MultipleBreathingP/0082998/0082998_rtstruct_ITV.nii.gz'\n",
    "    gtv = nib.load(gtv)\n",
    "    empty_header = nib.Nifti1Header()\n",
    "\n",
    "    for ct in CT_fpaths:\n",
    "        predicted_path = root_path+ct+'_'+w+'_predictedTS.nii.gz'\n",
    "        lung_path = ct[:-10]+'_LungMask.nii.gz'\n",
    "        #Get Tumor mask\n",
    "        #tumormask.mask(ct, predicted_path)\n",
    "        #Get Lung mask and save it\n",
    "        input_image = sitk.ReadImage(ct, imageIO='NiftiImageIO')\n",
    "        lungmask = lungmask_fun.apply(input_image)  # default model is U-net(R231)\n",
    "        lungmask_ni = nib.Nifti1Image(lungmask, gtv.affine,empty_header)\n",
    "        nib.save(lungmask_ni,lung_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check max min tumor values\n",
    "if False :\n",
    "    minmin = 10\n",
    "    maxmax = 0\n",
    "    for i in range(len(data_dicts)):\n",
    "        img = nib.load(data_dicts[i][\"lung\"])\n",
    "        data_img = img.get_fdata()\n",
    "        data_img = np.transpose(data_img, (1,2,0))# fix misshape \n",
    "        for j in range(data_img.shape[2]):\n",
    "            if np.sum(data_img[:,:,j])>0:\n",
    "                plt.imshow(data_img[:,:,j])\n",
    "                plt.show()\n",
    "        break\n",
    "        if np.min(data_img) < minmin:\n",
    "          minmin = np.min(data_img)\n",
    "        if np.max(data_img) > maxmax:\n",
    "          maxmax = np.max(data_img)\n",
    "\n",
    "    print(\"minmin: \",minmin,\"maxmax: \", maxmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val len: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image': '/home/umcg/Desktop/AutomaticITV_code/MultipleBreathingP-OneDriveCopy/MultipleBreathingP/0045632/0045632_ 0%_ct.nii.gz',\n",
       " 'lung': '/home/umcg/Desktop/AutomaticITV_code/MultipleBreathingP-OneDriveCopy/MultipleBreathingP/0045632/0045632_ 0%_LungMask.nii.gz',\n",
       " 'agtv': '/home/umcg/Desktop/AutomaticITV_code/MultipleBreathingP-OneDriveCopy/MultipleBreathingP/0045632/0045632_ 0%_aGTV.nii.gz'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create data dictionary - ### ALL going for segmentation\n",
    "data_dicts = [\n",
    "    {\"image\": image_name,\"lung\":lung_name,\"agtv\":agtv_name}\n",
    "    for image_name,lung_name,agtv_name in zip(CT_fpaths,lung_fpaths,lbl_fpaths)\n",
    "]\n",
    "val_files = data_dicts[:]\n",
    "print('val len:',len(val_files))\n",
    "\n",
    "minmin_CT = -1024 #NBIA\n",
    "maxmax_CT = 3071 #NBIA\n",
    "val_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to transpose lung mask\n",
    "class Create_sequences(MapTransform):\n",
    "    def __init__(self, keys):\n",
    "        super().__init__(keys)\n",
    "        \n",
    "        print(f\"keys to transpose: {self.keys}\")\n",
    "\n",
    "         \n",
    "    def __call__(self, dictionary):\n",
    "        dictionary = dict(dictionary)\n",
    "        for key in self.keys:\n",
    "            data = dictionary[key]\n",
    "            if key == 'lung':\n",
    "                data = np.transpose(data, (0,2,3,1))\n",
    "                data = rotate(data,270,axes=(1,2),reshape=False)\n",
    "                data = np.flip(data,1)\n",
    "                data[data==2] = int(1)\n",
    "                data[data!=1] = int(0)\n",
    "            dictionary[key] = data\n",
    "            \n",
    "        return dictionary        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys to transpose: ('image', 'lung')\n"
     ]
    }
   ],
   "source": [
    "#Create Compose functions for preprocessing of train and validation\n",
    "set_determinism(seed=0)\n",
    "image_keys = [\"image\",\"lung\"]\n",
    "p = .5 #Data aug transform probability\n",
    "size = 96\n",
    "image_size = (size,size,size)\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=image_keys),\n",
    "        #SqueezeDimd(keys=\"agtv\"),\n",
    "        EnsureChannelFirstd(keys=image_keys),\n",
    "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        #Spacingd(keys=[\"image\",\"agtv\"], pixdim=(1,1,1),mode=(\"bilinear\",\"nearest\")),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=minmin_CT, a_max=maxmax_CT,b_min=0.0, b_max=1.0, clip=True,),\n",
    "        Create_sequences(keys=image_keys),\n",
    "        CropForegroundd(keys=image_keys, source_key=\"lung\",k_divisible = size),\n",
    "        ToTensord(keys=image_keys),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys to transpose: ('agtv',)\n"
     ]
    }
   ],
   "source": [
    "#Create Compose functions for preprocessing of train and validation\n",
    "set_determinism(seed=0)\n",
    "image_keys = [\"agtv\"]\n",
    "p = .5 #Data aug transform probability\n",
    "size = 96\n",
    "image_size = (size,size,size)\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=image_keys),\n",
    "        #SqueezeDimd(keys=\"agtv\"),\n",
    "        EnsureChannelFirstd(keys=image_keys),\n",
    "        #Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        #Spacingd(keys=[\"image\",\"agtv\"], pixdim=(1,1,1),mode=(\"bilinear\",\"nearest\")),\n",
    "        #ScaleIntensityRanged(keys=[\"image\"], a_min=minmin_CT, a_max=maxmax_CT,b_min=0.0, b_max=1.0, clip=True,),\n",
    "        Create_sequences(keys=image_keys),\n",
    "        #CropForegroundd(keys=image_keys, source_key=\"lung\",k_divisible = size),\n",
    "        ToTensord(keys=image_keys),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "px info:1,agtv shape: (1, 288, 192)\n"
     ]
    }
   ],
   "source": [
    "#Check the images after the preprocessing\n",
    "if True:\n",
    "    check_ds =Dataset(data=val_files, transform=val_transforms)\n",
    "    check_loader = DataLoader(check_ds, batch_size=1,num_workers=0)\n",
    "    if True:\n",
    "        count = 1\n",
    "        #for batch_data in check_loader:\n",
    "        batch_data = first(check_loader)\n",
    "        agtv = (batch_data[\"agtv\"][0][0])\n",
    "        #image,lung,agtv = (batch_data[\"image\"][0][0],batch_data[\"lung\"][0][0],batch_data[\"agtv\"][0][0])\n",
    "        #print(batch_data[\"image\"].meta[\"filename_or_obj\"][0])\n",
    "        #print(f\"px info:{count },image shape: {image.shape},lung shape: {lung.shape}\")\n",
    "        print(f\"px info:{count },agtv shape: {agtv.shape}\")\n",
    "        count+=1\n",
    "        if False:\n",
    "            for i in range(image.shape[2]):\n",
    "                plt.subplot(1,3,1),plt.imshow(image[:,:,i]),plt.axis('off')\n",
    "                plt.subplot(1,3,2),plt.imshow(image[:,:,i]+lung[:,:,i]),plt.axis('off')\n",
    "                plt.subplot(1,3,3),plt.imshow(image[:,:,i]+agtv[:,:,i]),plt.axis('off')\n",
    "                count+=1\n",
    "                plt.tight_layout(),plt.show()\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the dataset ready for the model\n",
    "if False:\n",
    "    train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "    val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, num_workers=0)#,collate_fn=pad_list_data_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create the model\n",
    "spatial_dims = 3\n",
    "max_epochs = 100\n",
    "in_channels = 1\n",
    "out_channels=2 #including background\n",
    "if True:\n",
    "    model = SwinUNETR(\n",
    "        image_size, \n",
    "        in_channels, out_channels, \n",
    "        use_checkpoint=True, \n",
    "        feature_size=24,\n",
    "        #spatial_dims=spatial_dims\n",
    "    ).to(device)\n",
    "else:\n",
    "    model = UNet(\n",
    "        spatial_dims=spatial_dims,\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        strides=(2, 2, 2, 2),\n",
    "        num_res_units=4,\n",
    "        norm=Norm.BATCH,\n",
    "    ).to(device)\n",
    "\n",
    "#metrics\n",
    "loss_function = DiceLoss(to_onehot_y=True, sigmoid=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "surfDice_metric = SurfaceDiceMetric([0.01,0.01],include_background=True)\n",
    "surfDice_metric_1Class = SurfaceDiceMetric([3],include_background=False)\n",
    "hausdorff_metric = HausdorffDistanceMetric(include_background=True, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "pretrained_path = '/home/umcg/Desktop/AutomaticITV_code/weights/best_m_MONAI_V3_NBIAWeightsretrainedWithUMCGdata.pth'\n",
    "\n",
    "if pretrained_path is not(None):\n",
    "    model.load_state_dict(torch.load(pretrained_path, map_location=torch.device(device)))\n",
    "\n",
    "    #weight = torch.load(pretrained_path, map_location=torch.device(device))\n",
    "    #model.load_from(weights=weight)\n",
    "    print('Using pretrained weights!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_transforms = Compose(\n",
    "    [\n",
    "        EnsureType(),\n",
    "        AsDiscrete(argmax=True,threshold=.9),\n",
    "        FillHoles(applied_labels=1, connectivity=3),\n",
    "        RemoveSmallObjects(min_size=15, connectivity=3, independent_channels=False),\n",
    "        KeepLargestConnectedComponent(applied_labels=1,num_components=3),\n",
    "    ]\n",
    ")\n",
    "\n",
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True,threshold=0.5)])\n",
    "post_label = Compose([EnsureType(), AsDiscrete(threshold=0.5)],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#compute metric for current iteration\n",
    "#Batch x Channel x Height x Width - [B,C,H,W] - 1, 384, 288, 192\n",
    "#Channel is number of classes\n",
    "def surfDiceFun_1Class(val_labels,postImg_monai):\n",
    "    ylabe_BCHW_neg = val_labels[0].cpu().numpy()\n",
    "    ylabe_BCHW_neg = ylabe_BCHW_neg*-1+1\n",
    "    ylabe_BCHW_pos = val_labels[0].cpu().numpy()\n",
    "    ylabe_BCHW = np.concatenate((ylabe_BCHW_neg, ylabe_BCHW_pos),0)\n",
    "    transpose_monai = Compose([Transpose([3, 0, 1, 2])])\n",
    "    ylabe_BCHW = transpose_monai(ylabe_BCHW)\n",
    "    ypred_BCHW = postImg_monai[0].permute(3, 0, 1, 2)\n",
    "    \n",
    "    #print(ylabe_BCHW.shape,ypred_BCHW.shape)\n",
    "    list_surf=[]\n",
    "    for i in range(ypred_BCHW.shape[0]-4):\n",
    "        if  np.sum(ylabe_BCHW[i:i+2,1,:,:])>0:# or np.sum(ypred_BCHW[i:i+2,0,:,:])>0:\n",
    "            surfDice_metric_1Class(y_pred=ypred_BCHW[i:i+2,:,:,:], y=ylabe_BCHW[i:i+2,1:,:,:])\n",
    "            list_surf.append(surfDice_metric_1Class.aggregate().item())\n",
    "            #print(list_surf[-1])\n",
    "            surfDice_metric_1Class.reset()\n",
    "    return np.mean(list_surf)\n",
    "#hausdorff_metric(y_pred=postImg_monai, y=val_labels)\n",
    "#print('hausdorff monai    :',hausdorff_metric.aggregate().item())\n",
    "#hausdorff_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create headers to save the predictions\n",
    "gtv = '/home/umcg/Desktop/AutomaticITV_code/MultipleBreathingP-OneDriveCopy/MultipleBreathingP/0082998/0082998_rtstruct_ITV.nii.gz'\n",
    "gtv = nib.load(gtv)\n",
    "empty_header = nib.Nifti1Header()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing the model,no label used \n",
    "count=0\n",
    "nr_images=6\n",
    "figsize = (18, 12)\n",
    "#figures_folder_i = '/data/p308104/NBIA_Data/NIFTI_NBIA/Res-16092022/' #Peregrine\n",
    "figures_folder_i ='/home/umcg/Desktop/AutomaticITV_code/figures_folder_i/' #Local\n",
    "all_metrics = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for val_data in check_loader:\n",
    "        val_inputs = (val_data[\"image\"].to(device))\n",
    "        roi_size = image_size\n",
    "        sw_batch_size = 1\n",
    "        val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "        \n",
    "        #postImg_manual  = PostProcessing(val_outputs.cpu())\n",
    "        #postImg_manual  = [post_transforms(i) for i in decollate_batch(postImg_manual)]\n",
    "        postImg_monai = [post_transforms(i) for i in decollate_batch(val_outputs)]\n",
    "        val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "        px = val_data[\"image\"].meta[\"filename_or_obj\"][0].split('/')[-2]\n",
    "        newGTV_name = val_data[\"image\"].meta[\"filename_or_obj\"][0][:-9]+'aGTV.nii.gz'\n",
    "        bp = val_data[\"image\"].meta[\"filename_or_obj\"][0].split('%')[-2][-2:]\n",
    "        print(newGTV_name)\n",
    "\n",
    "        if True: #AXIAL\n",
    "            slice_indices = []   \n",
    "            for i in range(val_inputs.shape[4]):\n",
    "                if np.sum(val_outputs[0][0,:,:,i])>0:\n",
    "                    slice_indices.append(i)\n",
    "            if len(slice_indices) <nr_images:\n",
    "                slice_indices.append(random.sample(range(1, val_inputs.shape[4]),nr_images-len(slice_indices))[0])\n",
    "            else:\n",
    "                slice_indices = random.sample(slice_indices, k=nr_images)\n",
    "            instance = random.randint(0, val_inputs.shape[0] - 1)\n",
    "            j = 1\n",
    "            for i, idx in enumerate(slice_indices):\n",
    "                j=1+i\n",
    "                fig = plt.figure('Instance = {}'.format(instance), figsize=figsize)\n",
    "                plt.subplot(3, nr_images, j),plt.title('AXIAL CT ({})'.format(idx)),plt.imshow(val_inputs.cpu().numpy()[instance, 0, :, :, idx], cmap='gray', vmin=0, vmax=1),plt.axis('off')\n",
    "                plt.subplot(3, nr_images, nr_images+j),plt.title('Prediction ({})'.format(idx)),plt.imshow(val_inputs.cpu().numpy()[instance, 0, :, :, idx]+val_outputs[0].detach().cpu()[instance, :, :, idx]),plt.axis('off')\n",
    "                plt.subplot(3, nr_images, 2*nr_images+j),plt.title('postImg Monai ({})'.format(idx)),plt.imshow(val_inputs.cpu().numpy()[instance, 0, :, :, idx]+postImg_monai[0].detach().cpu()[instance, :, :, idx]),plt.axis('off')\n",
    "            plt.tight_layout(),plt.show()\n",
    "            if not os.path.exists(os.path.join(figures_folder_i,px)):\n",
    "                os.makedirs(os.path.join(figures_folder_i,px)) \n",
    "            plt.savefig(os.path.join(figures_folder_i,px,bp+'AXIAL final_{}.png'.format(px)))\n",
    "        if True: #SAGITAL\n",
    "            slice_indices = []   \n",
    "            for i in range(val_inputs.shape[3]):\n",
    "                if np.sum(val_outputs[0][0,:,i,:])>0:\n",
    "                    slice_indices.append(i)\n",
    "            if len(slice_indices) <nr_images:\n",
    "                slice_indices.append(random.sample(range(1, val_inputs.shape[3]),nr_images-len(slice_indices))[0])\n",
    "            else:\n",
    "                slice_indices = random.sample(slice_indices, k=nr_images)\n",
    "            instance = random.randint(0, val_inputs.shape[0] - 1)\n",
    "            j = 1\n",
    "            for i, idx in enumerate(slice_indices):\n",
    "                j=1+i\n",
    "                fig = plt.figure('Instance = {}'.format(instance), figsize=figsize)\n",
    "                plt.subplot(3, nr_images, j),plt.title('SAGITAL CT ({})'.format(idx)),plt.imshow(val_inputs.cpu().numpy()[instance, 0, :, idx,:], cmap='gray', vmin=0, vmax=1),plt.axis('off')\n",
    "                plt.subplot(3, nr_images, nr_images+j),plt.title('Prediction ({})'.format(idx)),plt.imshow(val_inputs.cpu().numpy()[instance, 0, :, idx,:]+val_outputs[0].detach().cpu()[instance, :,idx,:]),plt.axis('off')\n",
    "                plt.subplot(3, nr_images, 2*nr_images+j),plt.title('postImg Monai ({})'.format(idx)),plt.imshow(val_inputs.cpu().numpy()[instance, 0, :, idx,:]+postImg_monai[0].detach().cpu()[instance, :,idx,:]),plt.axis('off')\n",
    "            plt.tight_layout(),plt.show()\n",
    "            if not os.path.exists(os.path.join(figures_folder_i,px)):\n",
    "                os.makedirs(os.path.join(figures_folder_i,px)) \n",
    "            plt.savefig(os.path.join(figures_folder_i,px,bp+'SAGITAL final_{}.png'.format(px)))\n",
    "        if True: #CORONAL\n",
    "            slice_indices = []   \n",
    "            for i in range(val_inputs.shape[2]):\n",
    "                if np.sum(val_outputs[0][0,i,:,:])>0:\n",
    "                    slice_indices.append(i)\n",
    "            if len(slice_indices) <nr_images:\n",
    "                slice_indices.append(random.sample(range(1, val_inputs.shape[2]),nr_images-len(slice_indices))[0])\n",
    "            else:\n",
    "                slice_indices = random.sample(slice_indices, k=nr_images)\n",
    "            instance = random.randint(0, val_inputs.shape[0] - 1)\n",
    "            j = 1\n",
    "            for i, idx in enumerate(slice_indices):\n",
    "                j=1+i\n",
    "                fig = plt.figure('Instance = {}'.format(instance), figsize=figsize)\n",
    "                plt.subplot(3, nr_images, j),plt.title('CORONAL CT ({})'.format(idx)),plt.imshow(val_inputs.cpu().numpy()[instance, 0, idx,:,:], cmap='gray', vmin=0, vmax=1),plt.axis('off')\n",
    "                plt.subplot(3, nr_images, nr_images+j),plt.title('Prediction ({})'.format(idx)),plt.imshow(val_inputs.cpu().numpy()[instance, 0, idx,:,:]+val_outputs[0].detach().cpu()[instance,idx,:,:]),plt.axis('off')\n",
    "                plt.subplot(3, nr_images, 2*nr_images+j),plt.title('postImg Monai ({})'.format(idx)),plt.imshow(val_inputs.cpu().numpy()[instance, 0,idx,:,:]+postImg_monai[0].detach().cpu()[instance,idx,:,:]),plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            if not os.path.exists(os.path.join(figures_folder_i,px)):\n",
    "                os.makedirs(os.path.join(figures_folder_i,px)) \n",
    "            plt.savefig(os.path.join(figures_folder_i,px,bp+'CORONAL final_{}.png'.format(px)))\n",
    "        \n",
    "        #No METRICS - compute metric for current iteration\n",
    "        \n",
    "        predicted_ni = nib.Nifti1Image(postImg_monai[0].cpu().numpy(), gtv.affine,empty_header)\n",
    "        nib.save(predicted_ni,newGTV_name)\n",
    "        count+=1\n",
    "        if count>9:\n",
    "            break\n",
    "#NO METRICS  - Save csv of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train with lung mask \n",
    "#to create the ITV - sum the bp gtv and maybe give weights to some phases\n",
    "#use tumor mask to also create the ITV \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load csv of metrics\n",
    "\n",
    "if False:\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(r'/home/umcg/Desktop/AutomaticITV_code/figures_folder_i/res.csv')\n",
    "    print(df.mean())\n",
    "    print(df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "      hausdorff_metric(y_pred=postImg_monai, y=val_labels)\n",
    "        print('hausdorff monai    :',hausdorff_metric.aggregate().item())\n",
    "        hausdorff_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import dilation,disk,erosion\n",
    "def PostProcessing(predicted):\n",
    "    img = predicted\n",
    "    for i in range(predicted.shape[-1]-10):\n",
    "        j=i+5\n",
    "        #Get imgs\n",
    "        Before = predicted[0,1,:,:,j-1]\n",
    "        Current = predicted[0,1,:,:,j]\n",
    "        After = predicted[0,1,:,:,j+1]\n",
    "        AAfter = predicted[0,1,:,:,j+2]+After\n",
    "        AAfter = predicted[0,1,:,:,j+3]+AAfter\n",
    "        AAfter = predicted[0,1,:,:,j+4]+AAfter\n",
    "        AAfter = predicted[0,1,:,:,j+5]+AAfter\n",
    "        BBefore = predicted[0,1,:,:,j-2]+Before\n",
    "        BBefore = predicted[0,1,:,:,j-3]+BBefore\n",
    "        BBefore = predicted[0,1,:,:,j-4]+BBefore\n",
    "        BBefore = predicted[0,1,:,:,j-5]+BBefore\n",
    "        \n",
    "        #Binarize\n",
    "        t = .5\n",
    "        BBefore[BBefore>=t] =int(1)\n",
    "        BBefore[BBefore<t] =int(0)\n",
    "        Current[Current>=t] =int(1)\n",
    "        Current[Current<t] =int(0)\n",
    "        AAfter[AAfter>=t] =int(1)\n",
    "        AAfter[AAfter<t] =int(0)\n",
    "        \n",
    "        #Morphology\n",
    "        m = 15\n",
    "        BBefore = dilation(BBefore, disk(m))\n",
    "        Current = dilation(Current, disk(m))\n",
    "        AAfter = dilation(AAfter, disk(m))\n",
    "        m = 5\n",
    "        BBefore = erosion(BBefore, disk(m))\n",
    "        Current = erosion(Current, disk(m))\n",
    "        AAfter = erosion(AAfter, disk(m))\n",
    "        \n",
    "        #And Between slices -> <- ... for missing \n",
    "        ABS = np.logical_and(BBefore,AAfter)\n",
    "        Current = np.logical_or(ABS,Current)\n",
    "        \n",
    "        #And outside slices <-->  ... for false positives with RegionProps\n",
    "        BBefore_label = label(BBefore)\n",
    "        Current_label = label(Current)\n",
    "        AAfter_label = label(AAfter)\n",
    "        BBefore_regions = regionprops(BBefore_label)\n",
    "        Current_regions = regionprops(Current_label)\n",
    "        AAfter_regions = regionprops(AAfter_label)\n",
    "        #Look for centroids distance\n",
    "        limit =  20\n",
    "        if False:\n",
    "            for c_centroid in Current_regions:\n",
    "                for b_centroid in BBefore_regions:\n",
    "                    dif = np.abs(np.asarray(c_centroid['centroid'])-np.asarray(b_centroid['centroid']))\n",
    "                    if dif[0] > limit or dif[1] > limit: \n",
    "                        Current[Current==c_centroid['label']] = int(0)\n",
    "                        print('delete blob')\n",
    "                for a_centroid in AAfter_regions:\n",
    "                    dif = np.abs(np.asarray(c_centroid['centroid'])-np.asarray(a_centroid['centroid']))\n",
    "                    if dif[0] > limit or dif[1] > limit: \n",
    "                        Current[Current==c_centroid['label']] = int(0)\n",
    "                        print('delete blob')\n",
    "        if False:\n",
    "            plt.subplot(1,4,1),plt.imshow(BBefore,cmap='gray', vmin=0, vmax=1)\n",
    "            plt.subplot(1,4,2),plt.imshow(Current,cmap='gray', vmin=0, vmax=1)\n",
    "            plt.subplot(1,4,3),plt.imshow(predicted[0,1,:,:,j],cmap='gray', vmin=0, vmax=1)\n",
    "            plt.subplot(1,4,4),plt.imshow(AAfter,cmap='gray', vmin=0, vmax=1)\n",
    "            plt.show()\n",
    "    \n",
    "        img[0,0,:,:,j] = torch.from_numpy(Current)\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                if \"20%_ct.nii.gz\" in f.lower():\n",
    "                    CT20_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"20%_lungmask.nii.gz\" in f.lower():\n",
    "                    Lung20_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"30%_ct.nii.gz\" in f.lower():\n",
    "                    CT30_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"30%_lungmask.nii.gz\" in f.lower():\n",
    "                    Lung30_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"40%_ct.nii.gz\" in f.lower():\n",
    "                    CT40_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"40%_lungmask.nii.gz\" in f.lower():\n",
    "                    Lung40_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if  \"50%_ct.nii.gz\" in f.lower():\n",
    "                    CT50_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"50%_lungmask.nii.gz\" in f.lower():\n",
    "                    Lung50_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"60%_ct.nii.gz\" in f.lower():\n",
    "                    CT60_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"60%_lungmask.nii.gz\" in f.lower():\n",
    "                    Lung60_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"70%_ct.nii.gz\" in f.lower():\n",
    "                    CT70_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"70%_lungmask.nii.gz\" in f.lower():\n",
    "                    Lung70_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"80%_ct.nii.gz\" in f.lower():\n",
    "                    CT80_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"80%_lungmask.nii.gz\" in f.lower():\n",
    "                    Lung80_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"90%_ct.nii.gz\" in f.lower():\n",
    "                    CT90_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"90%_lungmask.nii.gz\" in f.lower():\n",
    "                    Lung90_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"_ 0%_ct.nii.gz\" in f.lower():\n",
    "                    CT100_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"_ 0%_lungmask.nii.gz\" in f.lower():\n",
    "                    Lung100_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1\n",
    "                if \"itv.nii.gz\" in f.lower():\n",
    "                    lbl_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    flag_PxOk+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
