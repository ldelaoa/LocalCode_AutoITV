{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afaf3366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "import csv\n",
    "import SimpleITK as sitk\n",
    "#from lungtumormask import mask as tumormask\n",
    "#from lungmask import mask as lungmask_fun\n",
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    RandFlipd,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    KeepLargestConnectedComponent,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    CenterSpatialCropd,\n",
    "    SpatialCropd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    AsDiscrete,\n",
    "    SpatialCrop,\n",
    "    RandSpatialCropd,\n",
    "    SpatialPadd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd,\n",
    "    DivisiblePadd,\n",
    "    MapTransform,\n",
    "    HistogramNormalized,\n",
    "    ToTensord,\n",
    "    Transpose,\n",
    "    ToTensor,\n",
    ")\n",
    "from monai.optimizers import LearningRateFinder\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric,SurfaceDiceMetric,SurfaceDistanceMetric,HausdorffDistanceMetric\n",
    "from monai.losses import DiceLoss,DiceCELoss,MaskedDiceLoss,DiceFocalLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch,pad_list_data_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eed56ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "\n",
    "\n",
    "if True:\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017dd46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307\n"
     ]
    }
   ],
   "source": [
    "#peregrine\n",
    "#root_path = '/data/p308104/NBIA_Data/NIFTI_NBIA/imagesTr/'\n",
    "#root_path = '/data/p308104/Nifti_Imgs_V0/' #UMCG data on peregrine\n",
    "#root_path = '/data/p308104/MultipleBP/'\n",
    "#local\n",
    "root_path = '/home/umcg/Desktop/NBIA/NBIA_Nifti_v0/'\n",
    "#root_path = '/home/umcg/Desktop/Dicom_UMCG/MultipleBP/' \n",
    "#root_path = '/home/umcg/OneDrive/MultipleBreathingP/'\n",
    "#root_path = '/home/umcg/Desktop/AutomaticITV_code/MultipleBreathingP-OneDriveCopy/MultipleBreathingP/'\n",
    "\n",
    "all_patientdir = []\n",
    "all_patientdir = os.listdir(root_path)\n",
    "all_patientdir.sort()\n",
    "print(len(all_patientdir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26c8559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Presets\n",
    "num_workers = 6\n",
    "set_determinism(seed=0)\n",
    "image_keys = [\"image\",\"lung\",\"label\"]\n",
    "p = .5 #Data aug transform probability\n",
    "size = 96\n",
    "image_size = (size,size,size)\n",
    "batch_size=2\n",
    "pin_memory = True if num_workers > 0 else False  # Do not change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68c361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307 307 307\n",
      "/home/umcg/Desktop/NBIA/NBIA_Nifti_v0/LUNG1-046/LUNG1-046_ct.nii.gz\n",
      "/home/umcg/Desktop/NBIA/NBIA_Nifti_v0/LUNG1-046/LUNG1-046_ct_GTV-1.nii.gz\n",
      "/home/umcg/Desktop/NBIA/NBIA_Nifti_v0/LUNG1-046/LUNG1-046_xBP_LungMask.nii.gz\n"
     ]
    }
   ],
   "source": [
    "CT_fpaths = []\n",
    "lbl_fpaths= []\n",
    "lung_fpaths = []\n",
    "for patient_path in all_patientdir:\n",
    "    ct_miss = True\n",
    "    gtv_miss = True\n",
    "    lung_miss = True\n",
    "    for root, dirs, files in os.walk(root_path+patient_path, topdown=False):\n",
    "        for f in files:\n",
    "            if \"_ct.nii.gz\" in f.lower() and ct_miss:\n",
    "                CT_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                ct_miss = False\n",
    "            if \"gtv-1.nii.gz\" in f.lower():\n",
    "                lbl_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                gtv_miss =False\n",
    "            if 'lungmask.nii.gz' in f.lower():\n",
    "                lung_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                lung_miss = False\n",
    "        if ct_miss:\n",
    "            for f in files:\n",
    "                if \"ex_ct.nii.gz\" in f.lower() and ct_miss:\n",
    "                    CT_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    ct_miss = False\n",
    "                if 'ex_lungmask.nii.gz' in f.lower() and lung_miss:\n",
    "                        lung_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                        lung_miss = False\n",
    "        if gtv_miss and not(ct_miss):\n",
    "            CT_fpaths.pop()\n",
    "        if gtv_miss and not(lung_miss):\n",
    "            lung_fpaths.pop()\n",
    "        if not(gtv_miss) and (ct_miss):\n",
    "            print(root)\n",
    "            for f in files:\n",
    "                if \"mar_ct.nii.gz\" in f.lower() and ct_miss:\n",
    "                    CT_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    ct_miss = False\n",
    "                if \"in_ct.nii.gz\" in f.lower() and ct_miss:\n",
    "                    CT_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    ct_miss = False\n",
    "                if 'mar_lungmask.nii.gz' in f.lower() and lung_miss:\n",
    "                    lung_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    lung_miss = False\n",
    "                if 'in_lungmask.nii.gz' in f.lower() and lung_miss:\n",
    "                    lung_fpaths.append(os.path.join(root_path,patient_path,f))\n",
    "                    lung_miss = False\n",
    "                    \n",
    "            \n",
    "print(len(CT_fpaths),len(lbl_fpaths),len(lung_fpaths))\n",
    "CT_fpaths.sort()\n",
    "lbl_fpaths.sort()\n",
    "lung_fpaths.sort()\n",
    "\n",
    "print(CT_fpaths[44])\n",
    "print(lbl_fpaths[44])\n",
    "print(lung_fpaths[44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "648b584d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train val len: 30 - 5\n"
     ]
    }
   ],
   "source": [
    "#Create data dictionat\n",
    "data_dicts = [\n",
    "    {\"image\": image_name,\"lung\":lung_name,\"label\": label_name}\n",
    "    for image_name,lung_name,label_name in zip(CT_fpaths,lung_fpaths,lbl_fpaths)\n",
    "]\n",
    "train_files, val_files = data_dicts[:-277], data_dicts[-5:]\n",
    "print('train val len:',len(train_files),'-',len(val_files))\n",
    "\n",
    "minmin_CT = -1024 #NBIA\n",
    "maxmax_CT = 3071 #NBIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fad5974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to transpose lung mask\n",
    "class Create_sequences(MapTransform):\n",
    "    def __init__(self, keys):\n",
    "        super().__init__(keys)        \n",
    "        print(f\"keys to transpose: {self.keys}\")\n",
    "     \n",
    "    def __call__(self, dictionary):\n",
    "        dictionary = dict(dictionary)\n",
    "        for key in self.keys:\n",
    "            data = dictionary[key]\n",
    "            if key == 'lung':\n",
    "                data = np.transpose(data, (0,2,3,1))\n",
    "                data = rotate(data,270,axes=(1,2),reshape=False)\n",
    "                data = np.flip(data,1)\n",
    "                data[data==2] = int(1)\n",
    "                data[data!=1] = int(0)\n",
    "            dictionary[key] = data\n",
    "            \n",
    "        return dictionary        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60b6903f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys to transpose: ('image', 'lung', 'label')\n",
      "keys to transpose: ('image', 'lung', 'label')\n"
     ]
    }
   ],
   "source": [
    "#Create Compose functions for preprocessing of train and validation\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=image_keys),\n",
    "        EnsureChannelFirstd(keys=image_keys),\n",
    "        Orientationd(keys=[\"image\",\"label\"], axcodes=\"RAS\"),\n",
    "        #Spacingd(keys=[\"image\",\"label\"], pixdim=(1,1,1),mode=(\"bilinear\",\"nearest\")),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=minmin_CT, a_max=maxmax_CT,b_min=0.0, b_max=1.0, clip=True,),\n",
    "        Create_sequences(keys=image_keys),\n",
    "        CropForegroundd(keys=image_keys, source_key=\"lung\",k_divisible = size),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=image_keys,label_key='label',spatial_size=image_size,pos=1,neg=1,num_samples=2,\n",
    "            image_key='image',image_threshold=0,),\n",
    "        ToTensord(keys=image_keys),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=image_keys),\n",
    "        EnsureChannelFirstd(keys=image_keys),\n",
    "        Orientationd(keys=[\"image\",\"label\"], axcodes=\"RAS\"),\n",
    "        #Spacingd(keys=[\"image\",\"label\"], pixdim=(1,1,1),mode=(\"bilinear\",\"nearest\")),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=minmin_CT, a_max=maxmax_CT,b_min=0.0, b_max=1.0, clip=True,),\n",
    "        Create_sequences(keys=image_keys),\n",
    "        CropForegroundd(keys=image_keys, source_key=\"lung\",k_divisible = size),\n",
    "        ToTensord(keys=image_keys),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdb810a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████████████████████| 30/30 [01:00<00:00,  2.00s/it]\n",
      "Loading dataset: 100%|████████████████████████████| 5/5 [00:17<00:00,  3.41s/it]\n"
     ]
    }
   ],
   "source": [
    "#Check the images after the preprocessing\n",
    "if True:\n",
    "    train_ds = CacheDataset(data=train_files, transform=train_transforms,cache_rate=1.0,num_workers=num_workers)\n",
    "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    val_ds = CacheDataset(data=val_files, transform=val_transforms,cache_rate=1.0,num_workers=int(num_workers//2))\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, num_workers=int(num_workers//2),pin_memory=pin_memory)\n",
    "    \n",
    "else:\n",
    "    check_ds =CacheDataset(data=train_files, transform=train_transforms,cache_rate=1.0,num_workers=num_workers)\n",
    "    check_loader = DataLoader(check_ds, batch_size=1,num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4717fb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "px info:1,image shape: (96, 96, 96),lung shape: (96, 96, 96), label shape: (96, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    count = 1\n",
    "    for batch_data in train_loader:\n",
    "        #batch_data = first(check_loader)\n",
    "        image,lung, label = (batch_data[\"image\"][0][0],batch_data[\"lung\"][0][0],batch_data[\"label\"][0][0])\n",
    "        print(f\"px info:{count },image shape: {image.shape},lung shape: {lung.shape}, label shape: {label.shape}\")\n",
    "        count+=1\n",
    "        for i in range(label.shape[2]):\n",
    "            if torch.sum(label[:,:,i])>0:\n",
    "                plt.subplot(1,3,1),plt.imshow(image[:,:,i]),plt.axis('off')\n",
    "                plt.subplot(1,3,2),plt.imshow(label[:,:,i]),plt.axis('off')\n",
    "                plt.subplot(1,3,3),plt.imshow(label[:,:,i]+image[:,:,i]),plt.axis('off')\n",
    "                plt.tight_layout(),plt.show()\n",
    "                break\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaaddb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model\n",
    "spatial_dims = 3\n",
    "max_epochs = 100\n",
    "in_channels = 1\n",
    "out_channels=2 #including background\n",
    "weight_decay = 1e-5\n",
    "\n",
    "model = SwinUNETR(\n",
    "    image_size, \n",
    "    in_channels, out_channels, \n",
    "    use_checkpoint=True, \n",
    "    feature_size=24,\n",
    "    #spatial_dims=spatial_dims\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6df96134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "\n",
    "LF_Dice = DiceLoss(include_background=False,to_onehot_y=True, sigmoid=True)\n",
    "loss_function = LF_Dice\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea022e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing optimal learning rate:   0%|                   | 0/20 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.80 GiB (GPU 0; 7.93 GiB total capacity; 2.79 GiB already allocated; 1.48 GiB free; 5.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lower_lr,weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m      4\u001b[0m lr_finder \u001b[38;5;241m=\u001b[39m LearningRateFinder(model, optimizer, loss_function, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mlr_finder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrange_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupper_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m steepest_lr, _ \u001b[38;5;241m=\u001b[39m lr_finder\u001b[38;5;241m.\u001b[39mget_steepest_gradient()\n\u001b[1;32m      7\u001b[0m ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m15\u001b[39m), facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/AutomaticITV_code/Aitv_envV0/lib/python3.10/site-packages/monai/optimizers/lr_finder.py:343\u001b[0m, in \u001b[0;36mLearningRateFinder.range_test\u001b[0;34m(self, train_loader, val_loader, image_extractor, label_extractor, start_lr, end_lr, num_iter, step_mode, smooth_f, diverge_th, accumulation_steps, non_blocking_transfer, auto_reset)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing optimal learning rate, iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_iter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# Train on batch and retrieve loss\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking_transfer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking_transfer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loader:\n\u001b[1;32m    345\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate(val_iter, non_blocking_transfer\u001b[38;5;241m=\u001b[39mnon_blocking_transfer)\n",
      "File \u001b[0;32m~/Desktop/AutomaticITV_code/Aitv_envV0/lib/python3.10/site-packages/monai/optimizers/lr_finder.py:415\u001b[0m, in \u001b[0;36mLearningRateFinder._train_batch\u001b[0;34m(self, train_iter, accumulation_steps, non_blocking_transfer)\u001b[0m\n\u001b[1;32m    413\u001b[0m             scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/AutomaticITV_code/Aitv_envV0/lib/python3.10/site-packages/torch/_tensor.py:388\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor w.r.t. graph leaves.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs)\n",
      "File \u001b[0;32m~/Desktop/AutomaticITV_code/Aitv_envV0/lib/python3.10/site-packages/torch/overrides.py:1498\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1493\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1494\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[0;32m-> 1498\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desktop/AutomaticITV_code/Aitv_envV0/lib/python3.10/site-packages/monai/data/meta_tensor.py:249\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 249\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# we might have 1 or multiple outputs. Might be MetaTensor, might be something\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# else (e.g., `__repr__` returns a string).\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# Convert to list (if necessary), process, and at end remove list if one was added.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(torch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_types\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m ):\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# for torch.max(torch.tensor(1.0), dim=0), the return type is named-tuple like\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AutomaticITV_code/Aitv_envV0/lib/python3.10/site-packages/torch/_tensor.py:1121\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunction():\n\u001b[0;32m-> 1121\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/Desktop/AutomaticITV_code/Aitv_envV0/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AutomaticITV_code/Aitv_envV0/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AutomaticITV_code/Aitv_envV0/lib/python3.10/site-packages/torch/autograd/function.py:253\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AutomaticITV_code/Aitv_envV0/lib/python3.10/site-packages/torch/utils/checkpoint.py:130\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    126\u001b[0m     detached_inputs \u001b[38;5;241m=\u001b[39m detach_variable(\u001b[38;5;28mtuple\u001b[39m(inputs))\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad(), \\\n\u001b[1;32m    128\u001b[0m          torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mctx\u001b[38;5;241m.\u001b[39mgpu_autocast_kwargs), \\\n\u001b[1;32m    129\u001b[0m          torch\u001b[38;5;241m.\u001b[39mcpu\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mctx\u001b[38;5;241m.\u001b[39mcpu_autocast_kwargs):\n\u001b[0;32m--> 130\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdetached_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    133\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (outputs,)\n",
      "File \u001b[0;32m~/Desktop/AutomaticITV_code/Aitv_envV0/lib/python3.10/site-packages/monai/networks/nets/swin_unetr.py:608\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward_part1\u001b[0;34m(self, x, mask_matrix)\u001b[0m\n\u001b[1;32m    606\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    607\u001b[0m x_windows \u001b[38;5;241m=\u001b[39m window_partition(shifted_x, window_size)\n\u001b[0;32m--> 608\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m attn_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m(window_size \u001b[38;5;241m+\u001b[39m (c,)))\n\u001b[1;32m    610\u001b[0m shifted_x \u001b[38;5;241m=\u001b[39m window_reverse(attn_windows, window_size, dims)\n",
      "File \u001b[0;32m~/Desktop/AutomaticITV_code/Aitv_envV0/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/AutomaticITV_code/Aitv_envV0/lib/python3.10/site-packages/monai/networks/nets/swin_unetr.py:497\u001b[0m, in \u001b[0;36mWindowAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    493\u001b[0m relative_position_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_position_bias_table[\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_position_index\u001b[38;5;241m.\u001b[39mclone()[:n, :n]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    495\u001b[0m ]\u001b[38;5;241m.\u001b[39mreshape(n, n, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    496\u001b[0m relative_position_bias \u001b[38;5;241m=\u001b[39m relative_position_bias\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 497\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrelative_position_bias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     nw \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.80 GiB (GPU 0; 7.93 GiB total capacity; 2.79 GiB already allocated; 1.48 GiB free; 5.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "lower_lr, upper_lr = 1e-5, 1e-0\n",
    "optimizer = torch.optim.Adam(model.parameters(), lower_lr,weight_decay=weight_decay)\n",
    "lr_finder = LearningRateFinder(model, optimizer, loss_function, device=device)\n",
    "lr_finder.range_test(train_loader, val_loader, end_lr=upper_lr, num_iter=20)\n",
    "steepest_lr, _ = lr_finder.get_steepest_gradient()\n",
    "ax = plt.subplots(1, 1, figsize=(15, 15), facecolor=\"white\")[1]\n",
    "_ = lr_finder.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d43a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "metric_values = []\n",
    "nr_images = 8\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs,labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "            f\"train_loss: {loss.item():.4f}\")\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b76df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_range(data, wrapped_generator):\n",
    "    plt.ion()\n",
    "    for q in data.values():\n",
    "        for d in q.values():\n",
    "            if isinstance(d, dict):\n",
    "                ax = d[\"line\"].axes\n",
    "                ax.legend()\n",
    "                fig = ax.get_figure()\n",
    "    fig.show()\n",
    "\n",
    "    for i in wrapped_generator:\n",
    "        yield i\n",
    "        for q in data.values():\n",
    "            for d in q.values():\n",
    "                if isinstance(d, dict):\n",
    "                    d[\"line\"].set_data(d[\"x\"], d[\"y\"])\n",
    "                    ax = d[\"line\"].axes\n",
    "                    ax.legend()\n",
    "                    ax.relim()\n",
    "                    ax.autoscale_view()\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3322c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_optimizer_scheduler(d):\n",
    "    d[\"model\"] = get_new_net()\n",
    "\n",
    "    if \"lr_lims\" in d:\n",
    "        d[\"optimizer\"] = torch.optim.Adam(\n",
    "            d[\"model\"].parameters(), d[\"lr_lims\"][0]\n",
    "        )\n",
    "        d[\"scheduler\"] = torch.optim.lr_scheduler.CyclicLR(\n",
    "            d[\"optimizer\"],\n",
    "            base_lr=d[\"lr_lims\"][0],\n",
    "            max_lr=d[\"lr_lims\"][1],\n",
    "            step_size_up=d[\"step\"],\n",
    "            cycle_momentum=False,\n",
    "        )\n",
    "    elif \"lr_lim\" in d:\n",
    "        d[\"optimizer\"] = torch.optim.Adam(d[\"model\"].parameters(), d[\"lr_lim\"])\n",
    "    else:\n",
    "        d[\"optimizer\"] = torch.optim.Adam(d[\"model\"].parameters())\n",
    "\n",
    "\n",
    "def train(max_epochs, axes, data):\n",
    "    for d in data.keys():\n",
    "        get_model_optimizer_scheduler(data[d])\n",
    "\n",
    "        for q, i in enumerate([\"train\", \"auc\", \"acc\"]):\n",
    "            data[d][i] = {\"x\": [], \"y\": []}\n",
    "            (data[d][i][\"line\"],) = axes[q].plot(\n",
    "                data[d][i][\"x\"], data[d][i][\"y\"], label=d\n",
    "            )\n",
    "\n",
    "        val_interval = 1\n",
    "\n",
    "    for epoch in plot_range(data, trange(max_epochs)):\n",
    "\n",
    "        for d in data.keys():\n",
    "            data[d][\"epoch_loss\"] = 0\n",
    "        for batch_data in train_loader:\n",
    "            inputs = batch_data[\"image\"].to(device)\n",
    "            labels = batch_data[\"label\"].to(device)\n",
    "\n",
    "            for d in data.keys():\n",
    "                data[d][\"optimizer\"].zero_grad()\n",
    "                outputs = data[d][\"model\"](inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                loss.backward()\n",
    "                data[d][\"optimizer\"].step()\n",
    "                if \"scheduler\" in data[d]:\n",
    "                    data[d][\"scheduler\"].step()\n",
    "                data[d][\"epoch_loss\"] += loss.item()\n",
    "        for d in data.keys():\n",
    "            data[d][\"epoch_loss\"] /= len(train_loader)\n",
    "            data[d][\"train\"][\"x\"].append(epoch + 1)\n",
    "            data[d][\"train\"][\"y\"].append(data[d][\"epoch_loss\"])\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            with eval_mode(*[data[d][\"model\"] for d in data.keys()]):\n",
    "                for d in data:\n",
    "                    data[d][\"y_pred\"] = torch.tensor(\n",
    "                        [], dtype=torch.float32, device=device\n",
    "                    )\n",
    "                y = torch.tensor([], dtype=torch.long, device=device)\n",
    "                for val_data in val_loader:\n",
    "                    val_images = val_data[\"image\"].to(device)\n",
    "                    val_labels = val_data[\"label\"].to(device)\n",
    "                    for d in data:\n",
    "                        data[d][\"y_pred\"] = torch.cat(\n",
    "                            [data[d][\"y_pred\"], data[d][\"model\"](val_images)],\n",
    "                            dim=0,\n",
    "                        )\n",
    "                    y = torch.cat([y, val_labels], dim=0)\n",
    "\n",
    "                for d in data:\n",
    "                    y_onehot = [y_trans(i) for i in decollate_batch(y)]\n",
    "                    y_pred_act = [y_pred_trans(i).cpu() for i in decollate_batch(data[d][\"y_pred\"])]\n",
    "                    auc_metric(y_pred_act, y_onehot)\n",
    "                    auc_result = auc_metric.aggregate()\n",
    "                    auc_metric.reset()\n",
    "                    del y_pred_act, y_onehot\n",
    "                    data[d][\"auc\"][\"x\"].append(epoch + 1)\n",
    "                    data[d][\"auc\"][\"y\"].append(auc_result)\n",
    "\n",
    "                    acc_value = torch.eq(data[d][\"y_pred\"].argmax(dim=1), y)\n",
    "                    acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "                    data[d][\"acc\"][\"x\"].append(epoch + 1)\n",
    "                    data[d][\"acc\"][\"y\"].append(acc_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b8012",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 10), facecolor=\"white\")\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Train loss\")\n",
    "axes[1].set_ylabel(\"AUC\")\n",
    "axes[2].set_ylabel(\"ACC\")\n",
    "\n",
    "# In the paper referenced at the top of this notebook, a step\n",
    "# size of 8 times the number of iterations per epoch is suggested.\n",
    "step_size = 8 * len(train_loader)\n",
    "\n",
    "max_epochs = 100\n",
    "data = {}\n",
    "data[\"Default LR\"] = {}\n",
    "data[\"Steepest LR\"] = {\"lr_lim\": steepest_lr}\n",
    "data[\"Cyclical LR\"] = {\n",
    "    \"lr_lims\": (0.8 * steepest_lr, 1.2 * steepest_lr),\n",
    "    \"step\": step_size,\n",
    "}\n",
    "\n",
    "train(max_epochs, axes, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
