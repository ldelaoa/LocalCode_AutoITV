{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05f2bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "import csv\n",
    "import SimpleITK as sitk\n",
    "#from lungtumormask import mask as tumormask\n",
    "from lungmask import mask as lungmask_fun\n",
    "from skimage.measure import label, regionprops,shannon_entropy\n",
    "from skimage.morphology import dilation,ball,erosion,remove_small_objects\n",
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    SaveImage,\n",
    "    ResizeWithPadOrCropd,\n",
    "    MaskIntensityd,\n",
    "    ScaleIntensityd,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    ResizeWithPadOrCrop,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    FillHoles,\n",
    "    RemoveSmallObjects,\n",
    "    KeepLargestConnectedComponent,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    CenterSpatialCropd,\n",
    "    SpatialCropd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    AsDiscrete,\n",
    "    SpatialCrop,\n",
    "    RandSpatialCropd,\n",
    "    SpatialPadd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd,\n",
    "    DivisiblePadd,\n",
    "    MapTransform,\n",
    "    RandWeightedCropd,\n",
    "    ToTensord,\n",
    "    Transpose,\n",
    "    ScaleIntensity,\n",
    ")\n",
    "from monai.networks.nets import UNet,VNet,SwinUNETR,UNETR,DynUNet\n",
    "from monai.metrics import DiceMetric,SurfaceDiceMetric,HausdorffDistanceMetric,compute_surface_dice\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch,pad_list_data_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b3931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "plot in line\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "\n",
    "if True:\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    print('plot in line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9358167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to transpose lung mask\n",
    "class Create_sequences(MapTransform):\n",
    "    def __init__(self, keys):\n",
    "        super().__init__(keys)\n",
    "        print(f\"keys to transpose: {self.keys}\")\n",
    "\n",
    "    def __call__(self, dictionary):\n",
    "        dictionary = dict(dictionary)\n",
    "        for key in self.keys:\n",
    "            data = dictionary[key]\n",
    "            if key == 'lung':\n",
    "                data = np.transpose(data, (0, 2, 3, 1))\n",
    "                data = rotate(data, 270, axes=(1, 2), reshape=False)\n",
    "                data = np.flip(data, 1)\n",
    "                data[data == 2] = int(1)\n",
    "                data[data != 1] = int(0)\n",
    "            dictionary[key] = data\n",
    "\n",
    "        return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0eeab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernels_strides(patch_size, spacing):\n",
    "    sizes, spacings = patch_size, spacing\n",
    "    input_size = sizes\n",
    "    strides, kernels = [], []\n",
    "    while True:\n",
    "        spacing_ratio = [sp / min(spacings) for sp in spacings]\n",
    "        stride = [\n",
    "            2 if ratio <= 2 and size >= 8 else 1\n",
    "            for (ratio, size) in zip(spacing_ratio, sizes)\n",
    "        ]\n",
    "        kernel = [3 if ratio <= 2 else 1 for ratio in spacing_ratio]\n",
    "        if all(s == 1 for s in stride):\n",
    "            break\n",
    "        for idx, (i, j) in enumerate(zip(sizes, stride)):\n",
    "            if i % j != 0:\n",
    "                raise ValueError(\n",
    "                    f\"Patch size is not supported, please try to modify the size {input_size[idx]} in the spatial dimension {idx}.\"\n",
    "                )\n",
    "        sizes = [i / j for i, j in zip(sizes, stride)]\n",
    "        spacings = [i * j for i, j in zip(spacings, stride)]\n",
    "        kernels.append(kernel)\n",
    "        strides.append(stride)\n",
    "\n",
    "    strides.insert(0, len(spacings) * [1])\n",
    "    kernels.append(len(spacings) * [3])\n",
    "    return kernels, strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aff453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestRun1LFNonerun1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# root_path = '/data/p308104/Nifti_Imgs_V0/' #UMCG data on peregrine\n",
    "#root_path = '/data/p308104/MultipleBP/'\n",
    "#root_path = '/home/umcg/OneDrive/MultipleBreathingP/'\n",
    "#root_path = '/home/umcg/Desktop/AutomaticITV_code/SABR1322_Nifti/'\n",
    "root_path = '/home/umcg/Desktop/AutomaticITV_code/SABR1322_Nifti_AllBP_V2/'\n",
    "\n",
    "#preweight_path = '/data/p308104/weights/v9/'\n",
    "preweight_path = '/home/umcg/Desktop/AutomaticITV_code/weights/v11/'\n",
    "pretrained_path_Swin = preweight_path + 'best_SwinUnet_V11_UMCG_TestSet3.pth'\n",
    "pretrained_path_Dyn = preweight_path + 'best_DynUnet_V11_UMCG_TestSet3.pth'\n",
    "    \n",
    "figures_path = '/home/umcg/Desktop/AutomaticITV_code/figures_folder_i/'\n",
    "\n",
    "cache = False\n",
    "lf_select = None # NOT Needed for testing\n",
    "\n",
    "SelectModel = 1  # 0 Swin  - 1 Dyn\n",
    "figures_folder_i = figures_path+'figures_SwinDyn_V11_ITV'\n",
    "name_run = \"TestRun\" + str(SelectModel) + \"LF\" + str(lf_select) + \"run1\"\n",
    "print(name_run)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43820050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateLungMasks(root_path, CT_fpaths):\n",
    "    # Get Lung mask and save it\n",
    "    CT_path0 = CT_fpaths[0]\n",
    "    CT_nii = nib.load(CT_path0)\n",
    "    for ct in CT_fpaths:\n",
    "        empty_header = nib.Nifti1Header()\n",
    "        lung_path = ct[:-10] + '_LungMask.nii.gz'\n",
    "        print('Creating Lung Mask: ', lung_path)\n",
    "        input_image = sitk.ReadImage(ct, imageIO='NiftiImageIO')\n",
    "        lungmask = lungmask_fun.apply(input_image)  # default model is U-net(R231)\n",
    "        lungmask_ni = nib.Nifti1Image(lungmask, CT_nii.affine, empty_header)\n",
    "        nib.save(lungmask_ni, lung_path)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18000289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LookSortFiles(root_path, all_patientdir):\n",
    "    CTALL_fpaths =[]\n",
    "    lungALL_fpaths =[]\n",
    "    itv_fpaths = []\n",
    "    gtv_fpaths = []    \n",
    "    for patient_path in all_patientdir:\n",
    "        ct_miss = 0\n",
    "        gtv_miss = 0\n",
    "        itv_miss = 0\n",
    "        lung_miss = 0\n",
    "        print(patient_path)\n",
    "        for root, dirs, files in os.walk(root_path + patient_path, topdown=False):\n",
    "            for f in files:\n",
    "                if \"_gtv\" in f.lower():\n",
    "                    gtv_fpaths.append(os.path.join(root_path, patient_path, f))\n",
    "                    gtv_miss +=1\n",
    "                if \"_igtv\" in f.lower() or \"itv\" in f.lower():\n",
    "                    itv_fpaths.append(os.path.join(root_path, patient_path, f))\n",
    "                    itv_miss +=1\n",
    "                if \"0%\" in f.lower() and not(\"ave\" in f.lower()):\n",
    "                    if \"ct\" in f.lower():                            \n",
    "                        CTALL_fpaths.append(os.path.join(root_path, patient_path, f))\n",
    "                        ct_miss +=1\n",
    "                    if \"lung\" in f.lower():\n",
    "                        lungALL_fpaths.append(os.path.join(root_path, patient_path, f))\n",
    "                        lung_miss +=1\n",
    "            for i in range(len(CTALL_fpaths)-1):\n",
    "                \n",
    "                gtv_fpaths.append(itv_fpaths[-1])\n",
    "                itv_fpaths.append(itv_fpaths[-1])\n",
    "\n",
    "    print('ct: ',ct_miss,\"Lungs: \",lung_miss,\"GTV Miss: \", gtv_miss, \"ITV Miss: \",itv_miss)\n",
    "    if False:\n",
    "        CreateLungMasks(root_path,CTALL_fpaths)\n",
    "    \n",
    "    CTALL_fpaths = np.sort(CTALL_fpaths)\n",
    "    lungALL_fpaths = np.sort(lungALL_fpaths)\n",
    "    itv_fpaths  = np.sort(itv_fpaths)\n",
    "    gtv_fpaths = np.sort(gtv_fpaths)\n",
    "    return CTALL_fpaths, itv_fpaths,gtv_fpaths, lungALL_fpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ab1219",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 in TestRun1LFNonerun1 ['0070683/']\n",
      "0070683/\n",
      "ct:  10 Lungs:  10 GTV Miss:  0 ITV Miss:  2\n",
      "CT val len: 9\n"
     ]
    }
   ],
   "source": [
    "##MAIN\n",
    "px_ = '0070683/'\n",
    "all_patientdir = []\n",
    "all_patientdir.append(px_)\n",
    "#all_patientdir = os.listdir(root_path)\n",
    "all_patientdir.sort()\n",
    "print(len(all_patientdir),'in',name_run,all_patientdir)\n",
    "CTALL_fpaths, itv_fpaths,gtv_fpaths, lungALL_fpaths = LookSortFiles(root_path, all_patientdir)\n",
    "    \n",
    "#Create data dictionat\n",
    "data_dicts = [\n",
    "    {\"image\": image_name,\"lung\":lung_name,\"GTV\": gtv_name,\"ITV\":itv_name}\n",
    "    for image_name,lung_name,gtv_name,itv_name in zip(CTALL_fpaths,lungALL_fpaths,gtv_fpaths,itv_fpaths)\n",
    "]\n",
    "val_files =data_dicts[:]\n",
    "print('CT val len:',len(val_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "058ba1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys to transpose: ('image', 'lung', 'GTV', 'ITV')\n"
     ]
    }
   ],
   "source": [
    "num_workers=0\n",
    "# HU are -1000 air , 0 water , usually normal tissue are around 0, top values should be around 100, bones are around 1000\n",
    "minmin_CT = -1024\n",
    "maxmax_CT = 200 \n",
    "#Create Compose functions for preprocessing of train and validation\n",
    "set_determinism(seed=0)\n",
    "image_keys = [\"image\",\"lung\",\"GTV\",\"ITV\"]\n",
    "p = .5 #Data aug transform probability\n",
    "size = 96\n",
    "image_size = (size,size,size)\n",
    "pin_memory = True if num_workers > 0 else False  # Do not change\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=image_keys),\n",
    "        EnsureChannelFirstd(keys=image_keys),\n",
    "        Orientationd(keys=[\"image\",\"GTV\",\"ITV\"], axcodes=\"RAS\"),\n",
    "        #Spacingd(keys=[\"image\",\"label\"], pixdim=(1,1,1),mode=(\"bilinear\",\"nearest\")),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=minmin_CT, a_max=maxmax_CT,b_min=0.0, b_max=1.0, clip=True,),\n",
    "        Create_sequences(keys=image_keys),\n",
    "        CropForegroundd(keys=image_keys, source_key=\"lung\",k_divisible = size),\n",
    "        MaskIntensityd(keys=[\"image\"], mask_key=\"lung\"),\n",
    "        ToTensord(keys=image_keys),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Check the images after the preprocessing\n",
    "if cache:  # Cache\n",
    "    #train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=num_workers)\n",
    "    #train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=num_workers)\n",
    "    val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0,\n",
    "                          num_workers=int(num_workers // 2))\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, num_workers=int(num_workers // 2), pin_memory=pin_memory)\n",
    "else:\n",
    "    #train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "    #train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=0)\n",
    "    val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, num_workers=0)  # ,collate_fn=pad_list_data_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2837ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebb847bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check the images after the preprocessing\n",
    "if False:\n",
    "    figsize = (18, 9)\n",
    "    check_ds =Dataset(data=val_files, transform=val_transforms)\n",
    "    check_loader = DataLoader(check_ds, batch_size=1,num_workers=0)\n",
    "    for batch_data in check_loader:\n",
    "        #batch_data = first(check_loader)\n",
    "        image,lung, label_ITV = (batch_data[\"image\"][0][0],batch_data[\"lung\"][0][0],batch_data[\"ITV\"][0][0])\n",
    "        px = batch_data[\"image\"].to('cpu').meta[\"filename_or_obj\"][0].split('/')[-2]\n",
    "        print(\"Px:\", px)\n",
    "        print(f\"image shape: {image.shape},lung shape: {lung.shape}, label shape: {label_ITV.shape}\")\n",
    "        count = 0\n",
    "        for i in range(label_ITV.shape[-1]):\n",
    "            if torch.sum(label_ITV[:,:,i])>0:\n",
    "                count+=1\n",
    "                fig = plt.figure('Instance = {}'.format(0), figsize=figsize)\n",
    "                plt.subplot(1,2,1),plt.imshow(np.rot90(image[:,:,i]),cmap='gray'),plt.axis('off')\n",
    "                plt.subplot(1,2,2),plt.imshow(np.rot90(image[:,:,i]),cmap='gray'),plt.axis('off')\n",
    "                plt.contour(np.rot90(lung[:, :,i]),colors='yellow')\n",
    "                plt.contour(np.rot90(label_ITV[:,:,i]),colors='red')\n",
    "                plt.tight_layout(),plt.show()\n",
    "            if count >0: \n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49abbe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL SwinDyn\n",
      "Using Swin pretrained weights!\n",
      "Using Dyn pretrained weights!\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "spatial_dims = 3\n",
    "max_epochs = 250\n",
    "in_channels = 1\n",
    "out_channels = 2  # including background\n",
    "lr = 1e-3  # 1e-4\n",
    "weight_decay = 1e-5\n",
    "T_0 = 40  # Cosine scheduler\n",
    "\n",
    "task_id = \"06\"\n",
    "deep_supr_num = 1  # when is 3 shape of outputs/labels dont match\n",
    "patch_size = image_size\n",
    "spacing = [1, 1, 1]\n",
    "kernels, strides = get_kernels_strides(patch_size, spacing)\n",
    "\n",
    "print(\"MODEL SwinDyn\")\n",
    "modelSwin = SwinUNETR(\n",
    "    image_size,\n",
    "    in_channels, out_channels,\n",
    "    use_checkpoint=True,\n",
    "    feature_size=48,\n",
    "    # spatial_dims=spatial_dims\n",
    ").to(device)\n",
    "task_id = \"06\"\n",
    "modelDyn = DynUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    kernel_size=kernels,\n",
    "    strides=strides,\n",
    "    upsample_kernel_size=strides[1:],\n",
    "    norm_name=\"instance\",\n",
    "    deep_supervision=False,  # when is 3 shape of outputs/labels dont match\n",
    "    deep_supr_num=deep_supr_num,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "#metrics, no definition of :\n",
    "#NO Loss Function\n",
    "#NO Optimizer\n",
    "# Load pretrained model\n",
    "if pretrained_path_Swin is not(None):\n",
    "        modelSwin.load_state_dict(torch.load(pretrained_path_Swin, map_location=torch.device(device)))\n",
    "        print('Using Swin pretrained weights!')\n",
    "if pretrained_path_Dyn is not(None):\n",
    "    modelDyn.load_state_dict(torch.load(pretrained_path_Dyn, map_location=torch.device(device)))\n",
    "    print('Using Dyn pretrained weights!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcbb35a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Px:  BP:  4D thorax 2.0  2.0  Br38  3  0% iMAR_ct.nii.gz  Count:  0070683 1\n",
      "num de blobs predicted:  1\n",
      "Px:  BP:  4D thorax 2.0  2.0  Br38  3  10% iMAR_ct.nii.gz  Count:  0070683 2\n",
      "num de blobs predicted:  1\n",
      "Px:  BP:  4D thorax 2.0  2.0  Br38  3  20% iMAR_ct.nii.gz  Count:  0070683 3\n",
      "num de blobs predicted:  1\n",
      "Px:  BP:  4D thorax 2.0  2.0  Br38  3  30% iMAR_ct.nii.gz  Count:  0070683 4\n",
      "num de blobs predicted:  1\n",
      "Px:  BP:  4D thorax 2.0  2.0  Br38  3  40% iMAR_ct.nii.gz  Count:  0070683 5\n",
      "num de blobs predicted:  1\n",
      "Px:  BP:  4D thorax 2.0  2.0  Br38  3  50% iMAR_ct.nii.gz  Count:  0070683 6\n",
      "num de blobs predicted:  1\n",
      "Px:  BP:  4D thorax 2.0  2.0  Br38  3  60% iMAR_ct.nii.gz  Count:  0070683 7\n",
      "num de blobs predicted:  1\n",
      "Px:  BP:  4D thorax 2.0  2.0  Br38  3  70% iMAR_ct.nii.gz  Count:  0070683 8\n",
      "num de blobs predicted:  1\n",
      "Px:  BP:  4D thorax 2.0  2.0  Br38  3  80% iMAR_ct.nii.gz  Count:  0070683 9\n",
      "num de blobs predicted:  1\n"
     ]
    }
   ],
   "source": [
    "##TESTING\n",
    "#Define PostTranforms\n",
    "out_channels = 2  # including background\n",
    "post_transforms = Compose(\n",
    "    [\n",
    "        EnsureType(),\n",
    "        AsDiscrete(argmax=True, threshold=0.9),\n",
    "        #FillHoles(applied_labels=1, connectivity=0),\n",
    "        #RemoveSmallObjects(min_size=64, connectivity=3, independent_channels=True),\n",
    "        ScaleIntensity(minv=0.0, maxv=1.0),\n",
    "        KeepLargestConnectedComponent(applied_labels=None,is_onehot=False,connectivity=2,num_components=1),\n",
    "    ]\n",
    ")\n",
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, threshold=0.9),\n",
    "                     ScaleIntensity(minv=0.0, maxv=1.0)])\n",
    "\n",
    "# Testing the model\n",
    "print(len(val_loader))\n",
    "\n",
    "predicted_ITV = []\n",
    "predicted_ITV_nopost = []\n",
    "best_blob = True\n",
    "count=0\n",
    "modelSwin.eval()\n",
    "modelDyn.eval()\n",
    "with torch.no_grad():\n",
    "    for val_data in val_loader:\n",
    "    \n",
    "        #val_inputs, val_labels = (\n",
    "        #    val_data[\"image\"].to(device),\n",
    "        #    val_data[\"label\"].to(device),)\n",
    "        val_inputs = val_data[\"image\"].to(device)\n",
    "        roi_size = image_size\n",
    "        sw_batch_size = 1\n",
    "        px = val_data[\"image\"].to('cpu').meta[\"filename_or_obj\"][0].split('/')[-2]\n",
    "        bp = val_data[\"image\"].to('cpu').meta[\"filename_or_obj\"][0].split('/')[-1].split('=')[-1]\n",
    "        print('Px: ', 'BP: ',bp,\" Count: \",px,count+1)\n",
    "        count+=1\n",
    "\n",
    "        val_outputs_Swin = sliding_window_inference(val_inputs, roi_size, sw_batch_size, modelSwin)\n",
    "        val_outPost_Swin = [post_transforms(i) for i in decollate_batch(val_outputs_Swin)]\n",
    "        val_outputs_Swin = [post_pred(i) for i in decollate_batch(val_outputs_Swin)]\n",
    "\n",
    "        val_outputs_Dyn = sliding_window_inference(val_inputs, roi_size, sw_batch_size, modelDyn)\n",
    "        val_outPost_Dyn = [post_transforms(i) for i in decollate_batch(val_outputs_Dyn)]\n",
    "        val_outputs_Dyn = [post_pred(i) for i in decollate_batch(val_outputs_Dyn)]\n",
    "\n",
    "        val_outPost = torch.logical_or(val_outPost_Swin[0], val_outPost_Dyn[0])        \n",
    "        #val_outPost = torch.add(val_outPost_Swin[0], val_outPost_Dyn[0])        \n",
    "        \n",
    "        if best_blob: \n",
    "\n",
    "            out_3dnp = val_outPost[0].detach().cpu().numpy()\n",
    "            out_3dnp = out_3dnp.squeeze()\n",
    "            out_3dnp = dilation(out_3dnp, ball(2))\n",
    "            label_out_3dnp = label(out_3dnp)\n",
    "            props = regionprops(label_out_3dnp,val_inputs[0].detach().cpu().numpy().squeeze())\n",
    "\n",
    "            print(\"num de blobs predicted: \",len(props))\n",
    "            for n in range(len(props)):\n",
    "                r = props[n]\n",
    "                #print('prediction bbox',r.bbox,\"size\",len(r.coords))\n",
    "                patch = np.zeros(out_3dnp.shape)\n",
    "                for j in range(len(r.coords)): \n",
    "                    patch[r.coords[j][0],r.coords[j][1],r.coords[j][2]]=1\n",
    "                #Create different matrixes, one for each blob to send to metrics\n",
    "                predicted_blobn = np.zeros(out_3dnp.shape)  \n",
    "                predicted_blobn[label_out_3dnp==n+1]=1\n",
    "                #print(\"Bounding Box: \",r.bbox)\n",
    "                #print(\"Values: \",r.axis_major_length/r.axis_minor_length,\"Feret: \",r.feret_diameter_max)\n",
    "                #print(\"Intensity:\",r.intensity_max,r.intensity_min,r.intensity_mean)\n",
    "                #print(\"Entropy:\", shannon_entropy(predicted_blobn))\n",
    "\n",
    "                if len(props)==1:\n",
    "                    bestBlob = np.expand_dims(predicted_blobn, 0)\n",
    "                    tensor_blobn = torch.from_numpy(bestBlob)\n",
    "                elif n==0:\n",
    "                    ratio_zero = r.axis_major_length/r.axis_minor_length\n",
    "                    minint_zero = r.intensity_min\n",
    "                    entr_zero = shannon_entropy(predicted_blobn)\n",
    "                    blob_zero = predicted_blobn\n",
    "                elif n>0: \n",
    "                    ratio_curr = r.axis_major_length/r.axis_minor_length\n",
    "                    minint_curr = r.intensity_min\n",
    "                    entr_curr = shannon_entropy(predicted_blobn)\n",
    "                    if (ratio_curr>3) or (ratio_zero>3):\n",
    "                        print(\"Selected by ratio\")\n",
    "                        if ratio_curr<ratio_zero:\n",
    "                            bestBlob = np.expand_dims(predicted_blobn, 0)\n",
    "                            tensor_blobn = torch.from_numpy(bestBlob)\n",
    "                        else:\n",
    "                            bestBlob = np.expand_dims(blob_zero, 0)\n",
    "                            tensor_blobn = torch.from_numpy(bestBlob)\n",
    "                    else:\n",
    "                        if minint_zero<0.0001: \n",
    "                            bestBlob = np.expand_dims(predicted_blobn, 0)\n",
    "                            tensor_blobn = torch.from_numpy(bestBlob)\n",
    "                        elif minint_curr<0.0001: \n",
    "                            bestBlob = np.expand_dims(blob_zero, 0)\n",
    "                            tensor_blobn = torch.from_numpy(bestBlob)\n",
    "                        elif entr_curr<entr_zero:\n",
    "                            bestBlob = np.expand_dims(predicted_blobn, 0)\n",
    "                            tensor_blobn = torch.from_numpy(bestBlob)\n",
    "                        else:\n",
    "                            bestBlob = np.expand_dims(blob_zero, 0)\n",
    "                            tensor_blobn = torch.from_numpy(bestBlob)            \n",
    "            \n",
    "        if best_blob:\n",
    "            predicted_ITV.append(tensor_blobn)\n",
    "        else:\n",
    "            predicted_ITV.append(val_outPost)\n",
    "        predicted_ITV_nopost.append(val_outPost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2dc5d4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 384, 384, 192) (1, 384, 384, 192) (1, 384, 384, 192)\n",
      "(1, 384, 384, 192) (1, 384, 384, 192) (1, 384, 384, 192)\n"
     ]
    }
   ],
   "source": [
    "def rescaleITV(predicted_ITV):\n",
    "\n",
    "    predicted_ITV_rescale = []\n",
    "\n",
    "    post_rescale = Compose([ResizeWithPadOrCrop(spatial_size=(384,384,192),method=\"symmetric\"),AsDiscrete(threshold=0.1),ScaleIntensity(minv=0.0, maxv=1.0)])\n",
    "    for k in range(len(predicted_ITV)):\n",
    "        temp_ITV_tensor = post_rescale(predicted_ITV[k])\n",
    "        predicted_ITV_rescale.append(temp_ITV_tensor)\n",
    "\n",
    "        if k ==0: \n",
    "            ITV_tensor_10BP  = temp_ITV_tensor\n",
    "            ITV_tensor_2BP   = temp_ITV_tensor\n",
    "        elif k==4:\n",
    "            ITV_tensor_2BP   = torch.add(ITV_tensor_2BP, temp_ITV_tensor)\n",
    "        elif k!=0 and k!=5:\n",
    "            ITV_tensor_10BP  = torch.add(ITV_tensor_10BP, temp_ITV_tensor)\n",
    "\n",
    "\n",
    "        print(ITV_tensor_10BP.shape,ITV_tensor_2BP.shape,temp_ITV_tensor.shape)\n",
    "        return ITV_tensor_10BP,ITV_tensor_2BP\n",
    "\n",
    "ITV_tensor_10BP,ITV_tensor_2BP = rescaleITV(predicted_ITV)\n",
    "ITV_tensor_10BP_noPost,ITV_tensor_2BPnoPost = rescaleITV(predicted_ITV_nopost)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6afe089",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num de blobs in label:  1\n",
      "label bbox  (211, 133, 102, 236, 156, 115) size 3273\n",
      "torch.Size([1, 384, 384, 192])\n"
     ]
    }
   ],
   "source": [
    "post_label = Compose([EnsureType(),ResizeWithPadOrCrop(spatial_size=(384,384,192),method=\"symmetric\"),AsDiscrete(threshold=0.5),ScaleIntensity(minv=0.0, maxv=1.0)])\n",
    "#Create Label tensors\n",
    "#val_GTV,val_ITV = val_data[\"GTV\"].to(device),val_data[\"ITV\"].to(device)\n",
    "#val_GTV = [post_label(i) for i in decollate_batch(val_GTV)]\n",
    "val_ITV = val_data[\"ITV\"].to(device)\n",
    "val_ITV = [post_label(i) for i in decollate_batch(val_ITV)]\n",
    "\n",
    "lbl_3dnp = val_ITV[0].detach().cpu().numpy()\n",
    "lbl_3dnp = lbl_3dnp.squeeze()\n",
    "lbl_4d = np.expand_dims(lbl_3dnp, 0)\n",
    "tensor_label = torch.from_numpy(lbl_4d)\n",
    "label_img = label(lbl_3dnp)\n",
    "regions = regionprops(label_img)\n",
    "print(\"num de blobs in label: \",len(regions))\n",
    "for i in range(len(regions)):\n",
    "    r = regions[i]\n",
    "    print('label bbox ',r.bbox,\"size\",len(r.coords))\n",
    "    lbl_bbox = r.bbox\n",
    "    \n",
    "print(tensor_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ed98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44f8334f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "def saveNifti(np_image,path_to_save,filename):\n",
    "    if not os.path.exists(path_to_save):\n",
    "        os.makedirs(path_to_save)\n",
    "    converted_array = np.array(np_image, dtype=np.float32)\n",
    "    affine = np.eye(4)\n",
    "    nifti_file = nib.Nifti1Image(converted_array, affine)\n",
    "    nib.save(nifti_file, path_to_save+filename) # Here you put the path + the extionsion 'nii' or 'nii.gz'\n",
    "    return 0\n",
    "    \n",
    "#path_to_save = os.path.join(figures_folder_i, px)\n",
    "path_to_save = os.path.join(root_path, px)\n",
    "saveNifti(ITV_tensor_10BP, path_to_save,'/predictedITV_AllBP.Nii')\n",
    "saveNifti(ITV_tensor_10BP_noPost.detach().cpu().numpy(),path_to_save , '/predictedITV_AllBP_NoPost.Nii')\n",
    "saveNifti(tensor_label, path_to_save , '/ITV_Label.Nii')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5935e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TumorTrajectory(predicted_ITV_interp):\n",
    "    xx = np.zeros(10,int)\n",
    "    yy = np.zeros(10,int)\n",
    "    zz = np.zeros(10,int)\n",
    "    for l in range(len(predicted_ITV_interp)):\n",
    "        labeledGTV = label(predicted_ITV_interp[l].detach().cpu().numpy().squeeze())\n",
    "        propsGTV = regionprops(labeledGTV)\n",
    "        for m in range(len(propsGTV)):\n",
    "            #print(\"GTV #\",l,\" -Centroid: \",propsGTV[m].centroid)\n",
    "            xx[l],yy[l],zz[l] = propsGTV[m].centroid_local\n",
    "    ax = plt.figure().add_subplot(projection='3d')\n",
    "    plt.subplot(1,1,1),plt.plot(xx,yy,zz,label='x,y,z')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    plt.subplot(1,3,1),plt.plot(xx)\n",
    "    plt.subplot(1,3,2),plt.plot(yy)\n",
    "    plt.subplot(1,3,3),plt.plot(zz)\n",
    "    plt.show()\n",
    "    print('Limits: ',xx.max()-xx.min(),yy.max()-yy.min(),zz.max()-zz.min())\n",
    "    return\n",
    "TumorTrajectory(predicted_ITV_rescale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics\n",
    "def metrics(Metrics_tensor,tensor_label):\n",
    "    dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "    hausdorff_metric = HausdorffDistanceMetric(include_background=False, reduction=\"mean\", percentile=95)\n",
    "    surfDice_metric191 = SurfaceDiceMetric(class_thresholds=np.linspace(3, 3, 191), include_background=False)\n",
    "    surfDice_metric287 = SurfaceDiceMetric(class_thresholds=np.linspace(3,3,287), include_background=False)\n",
    "    surfDice_metric383 = SurfaceDiceMetric(class_thresholds=np.linspace(3, 3, 383), include_background=False)\n",
    "    \n",
    "    voxTP = 0\n",
    "    voxFN = 0\n",
    "    voxFP = 0\n",
    "\n",
    "    #DICE\n",
    "    dice_metric(y_pred=Metrics_tensor, y=tensor_label) #[0][1:2, :, :, :] [0]\n",
    "    dice1 = dice_metric.aggregate().item()\n",
    "    print('Dice :', dice1)\n",
    "    dice_metric.reset()\n",
    "\n",
    "    hausdorff_metric(y_pred=Metrics_tensor, y=tensor_label)\n",
    "    hausd1 = hausdorff_metric.aggregate().item()\n",
    "    hausdorff_metric.reset()\n",
    "    print('Hausdorff:', hausd1)\n",
    "\n",
    "    #print(\"Shape:\",tensor_blobn.shape,tensor_blobn[0].shape[1])\n",
    "    if Metrics_tensor.shape[1]==288:\n",
    "        surfDice_metric287(y_pred=Metrics_tensor, y=tensor_label)\n",
    "        sdice1 = surfDice_metric287.aggregate().item()\n",
    "        surfDice_metric287.reset()\n",
    "    elif Metrics_tensor.shape[1]==192:\n",
    "        surfDice_metric191(y_pred=Metrics_tensor, y=tensor_label)\n",
    "        sdice1 = surfDice_metric191.aggregate().item()\n",
    "        surfDice_metric191.reset()\n",
    "    else:\n",
    "        surfDice_metric383(y_pred=Metrics_tensor, y=tensor_label)\n",
    "        sdice1 = surfDice_metric383.aggregate().item()\n",
    "        surfDice_metric383.reset()\n",
    "    print('Surface dice:', sdice1)\n",
    "\n",
    "    out_3dnp = Metrics_tensor.detach().cpu().numpy()\n",
    "    out_3dnp = out_3dnp.squeeze()\n",
    "    label_out_3dnp = label(out_3dnp)\n",
    "    props = regionprops(label_out_3dnp)\n",
    "\n",
    "    lbl_3dnp = tensor_label.detach().cpu().numpy()\n",
    "    lbl_3dnp = lbl_3dnp.squeeze()\n",
    "    label_lbl = label(lbl_3dnp)\n",
    "    regions = regionprops(label_lbl)\n",
    "    print(\"num de blobs in label: \",len(regions))\n",
    "    for i in range(len(regions)):\n",
    "        r = regions[i]\n",
    "        lbl_bbox = r.bbox\n",
    "\n",
    "    print(\"num de blobs predicted: \",len(props))\n",
    "    for n in range(len(props)):\n",
    "        r = props[n]\n",
    "        print(\"Blob Area:\",r.area)\n",
    "        TP = False\n",
    "        for j in range(len(r.coords)):\n",
    "            if (r.coords[j][0]>lbl_bbox[0] and r.coords[j][0]<lbl_bbox[3]):\n",
    "                if (r.coords[j][1]>lbl_bbox[1] and r.coords[j][1]<lbl_bbox[4]):\n",
    "                    if (r.coords[j][2]>lbl_bbox[2] and r.coords[j][2]<lbl_bbox[5]):\n",
    "                        TP=True\n",
    "                        voxTP+=1\n",
    "        if TP:\n",
    "            sumPredicted =np.sum(out_3dnp)\n",
    "            sumGroundT=np.sum(lbl_3dnp)\n",
    "            voxFP=abs(sumPredicted-voxTP)\n",
    "            voxFN=abs(sumGroundT-voxTP)\n",
    "\n",
    "            print(\"Sensitivity: \",voxTP/(voxTP+voxFP))\n",
    "            print(\"Precision: \",voxTP/(voxTP+voxFN))\n",
    "            \n",
    "        \n",
    "        return out_3dnp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e7853d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def postITV(ITV_tensor,binarize):\n",
    "    tensor_blobn=[]\n",
    "    #Delete outliers of slices\n",
    "    out_3dnp = ITV_tensor.detach().cpu().numpy()\n",
    "    out_3dnp = out_3dnp.squeeze()\n",
    "    out_3dnp[:,:,:2] = 0\n",
    "    out_3dnp[:,:,180:] = 0\n",
    "    out_3dnp[out_3dnp>=1] = 1\n",
    "    out_3dnp[out_3dnp<1] = 0\n",
    "    label_out_3dnp = label(out_3dnp)\n",
    "    props = regionprops(label_out_3dnp)\n",
    "    for n in range(len(props)):\n",
    "        r = props[n]        \n",
    "        predicted_blobn = np.zeros(out_3dnp.shape)  \n",
    "        predicted_blobn[label_out_3dnp==n+1]=1\n",
    "        #predicted_blobn = dilation(predicted_blobn, ball(3))\n",
    "        predicted_blobn = np.expand_dims(predicted_blobn, 0)\n",
    "        tensor_blobn.append(torch.from_numpy(predicted_blobn))\n",
    "    return tensor_blobn\n",
    "\n",
    "print(\"All breathing phases:\")\n",
    "ITV_tensor_post = postITV(ITV_tensor_10BP,binarize=True)\n",
    "print(len(ITV_tensor_post),\" Blobs found\")\n",
    "for p in range(len(ITV_tensor_post)):\n",
    "    print(\"Stats for blob #\",p+1)\n",
    "    out_3dnp = metrics(ITV_tensor_post[p],tensor_label)\n",
    "    path_to_save = os.path.join(figures_folder_i, px)\n",
    "    saveNifti(np_image=out_3dnp, path_to_save=path_to_save +'/predictedITV_AllBP_'+str(p)+'.Nii')\n",
    "\n",
    "print('')\n",
    "print(\"TWO breathing phases:\")\n",
    "BP2_ITV_tensor_post = postITV(ITV_tensor_2BP,binarize=True)\n",
    "for p in range(len(BP2_ITV_tensor_post)):\n",
    "    out_3dnp = metrics(BP2_ITV_tensor_post[p],tensor_label)\n",
    "    path_to_save = os.path.join(figures_folder_i, px)\n",
    "    saveNifti(np_image=out_3dnp, path_to_save=path_to_save +'/predicted_TwoBP_'+str(p)+'.Nii')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b1f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plotOutput(ITV_Final_Tensor,predicted_ITV,tensor_label,name,px,minmin_CT_plot,maxmax_CT_plot):\n",
    "    post_rescale_image = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=image_keys),\n",
    "            EnsureChannelFirstd(keys=image_keys),\n",
    "            Orientationd(keys=[\"image\",\"GTV\",\"ITV\"], axcodes=\"RAS\"),\n",
    "            #Spacingd(keys=[\"image\",\"label\"], pixdim=(1,1,1),mode=(\"bilinear\",\"nearest\")),\n",
    "            ScaleIntensityRanged(keys=[\"image\"], a_min=minmin_CT_plot, a_max=maxmax_CT_plot,b_min=0.0, b_max=1.0, clip=True,),\n",
    "            Create_sequences(keys=image_keys),\n",
    "            CropForegroundd(keys=image_keys, source_key=\"lung\",k_divisible = size),\n",
    "            ResizeWithPadOrCropd(keys = [\"image\"],spatial_size=(384,384,192),mode=\"symmetric\"),\n",
    "            ToTensord(keys=image_keys),\n",
    "        ])\n",
    "    figsize = (18, 8)\n",
    "    #image\n",
    "    val_ds = Dataset(data=val_files, transform=post_rescale_image)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, num_workers=0)  # ,collate_fn=pad_list_data_collate)\n",
    "    batch_data = first(val_loader)\n",
    "    image,lung, label_ITV = (batch_data[\"image\"][0][0],batch_data[\"lung\"][0][0],batch_data[\"ITV\"][0][0])\n",
    "    image = image.detach().cpu().numpy()\n",
    "    image = image.squeeze()\n",
    "\n",
    "    #ITV\n",
    "    lblimg = post_rescale(tensor_label)\n",
    "    lblimg = lblimg.detach().cpu().numpy()\n",
    "    lblimg = lblimg.squeeze()\n",
    "\n",
    "\n",
    "    #Prediction\n",
    "    breathingphases = len(predicted_ITV)\n",
    "    out_3dnp = ITV_Final_Tensor.detach().cpu().numpy()\n",
    "    out_3dnp = out_3dnp.squeeze()\n",
    "\n",
    "\n",
    "    for i in range(lbl_3dnp.shape[2]):\n",
    "        if (np.sum(lbl_3dnp[:, :,i],)>0) or (np.sum(out_3dnp[:, :,i],)>0):\n",
    "                fig = plt.figure('Instance = {}'.format(0), figsize=figsize)\n",
    "                ax = fig.add_subplot(121)\n",
    "                ax.imshow(np.rot90(image[96:288,96:288, i]),cmap='gray'),plt.axis('off')\n",
    "                ax = fig.add_subplot(122)\n",
    "                ax.imshow(np.rot90(image[96:288,96:288, i]),cmap='gray'),plt.axis('off')\n",
    "                for bp in range(breathingphases):\n",
    "                    ax.contour(np.rot90(predicted_ITV[bp].detach().cpu().numpy()[0,96:288,96:288,i]),colors='red')\n",
    "                ax.contour(np.rot90(lblimg[96:288,96:288,i]),colors='yellow')\n",
    "                ax.contour(np.rot90(out_3dnp[96:288,96:288,i]),colors='blue')\n",
    "                ax.text(8, 10, 'Yellow ITV Label', style='normal',color='white',fontsize=15)\n",
    "                ax.text(8, 25, 'Blue ITV Prediction', style='normal',color='white',fontsize=15)\n",
    "                ax.text(8, 40, ['Red GTV Prediction',i], style='normal',color='white',fontsize=15)\n",
    "                plt.show()\n",
    "                #if not os.path.exists(os.path.join(figures_folder_i, px)):\n",
    "                #                os.makedirs(os.path.join(figures_folder_i, px))\n",
    "                #plt.savefig(os.path.join(figures_folder_i, px, 'FullV_final_V11{}.png'.format(i,name)))\n",
    "                #plt.clf()\n",
    "    return 0\n",
    "minmin_CT_plot = -1024\n",
    "maxmax_CT_plot = 2000\n",
    "plotOutput(ITV_tensor_post[0],predicted_ITV_rescale,tensor_label,\"FullBP\",px,minmin_CT_plot,maxmax_CT_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071502d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figures_folder_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8fb900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219521a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
